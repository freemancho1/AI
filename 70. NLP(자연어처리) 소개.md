# NLP(Natural Language Processing, 자연어처리)
<br/>

> 자연어(natual language)는 인간이 일상생활을 하면서 사용하는 언어를 말하며, **자연어 처리란 자연어를 컴퓨터가 이해하고 활용할 수 있도록 하는 인공지능의 한 분야**다. <br/>
> 컴퓨터가 자연어를 이해하고 활용한다는 말은 인간이 말로 뭔가를 시키면 컴퓨터가 인간의 말을 이해해 일을 수행하거나, 인간이 뭔가를 물어보면 해당 질문을 이해하고 거기에 적절한 답변을 하는 것 등을 말한다.

<br/>

> 자연어 처리를 위해서는 크게 5가지 분야로 생각해 볼 수 있다.

##### ⓐ 단어 표현
##### ⓑ 텍스트 분류
##### ⓒ 텍스트 유사도
##### ⓓ 텍스트 생성
##### ⓔ 기계 이해

<br/>

> 계속하는 말이지만 모든 인공지능 분야에서 데이터를 이해하는 것은 매우 중요하다.  마찬가지로, 자연어 처리 분야에서도 기본 데이터인 언어에 대한 이해가 매우 중요하다.

<br/><br/>

## 1. 단어 표현 = 단어 임베딩(word embedding) = 단어 벡터(word vector)

<br/>

### 1.1. 개념
> 단어 표현이란 인간의 언어를 컴퓨터가 이해할 수 있도록 표현하는 분야로, 텍스트를 자연어 처리 모델에 적용 할 수 있도록 **언어적인 특성을 반영해 단어를 수치화하는 방법**을 말한다.

<br/>

### 1.2. 단어 표현 방법
> 단어 표현 방법은 주로 아래 3가지 방법을 사용한다.
> * 원-핫 인코딩(One-Hot encoding)
> * 카운팅 방법
> * 예측 방법
> * Glove (카운팅방법 + 예측방법)

<br/>

#### 1.2.1. 원-핫 인코딩(One-Hot encoding)
> 단어를 0과 1로 구성된 하나의 벡터로 구성하는 가장 기본적인 단어 표현 방법이다. <br/>
> 단어의 갯수만큼 0으로 구성된 벡터를 만들고, 각각 하나씩 1을 부여해 단어의 의미를 부여하는 방법이다.

##### 장점
> * 매우 직관적이고 쉽다.

##### 단점
> * 자연어 처리는 수백만개의 단어를 처리하는데 이때 벡터가 너무 커지고, 실제 사용하는 1이 하나만 존재하기 때문에 공간이 비효율적이다.
> * 단어 자체가 가지는 의미나 특성을 전혀 표현하지 못한다.

<br/>

#### 1.2.2. 카운트 기반(Count-Base) 방법 (많이 사용하는 방법)
> 특정 문맥 안에서 단어들이 동시에 등장하는 횟수를 직접 세는 방법이다. <br/>
> * A단어, B단어 동시 등장하는 횟수를 말함

##### 구현방법
> * 특이값 분해(Singular Value Decomposition, SVD)
> * 잠재의미 분석(Latent Semantic Analysis, LSA)
> * Hyper-space Analogue to Language(HAL)
> * Hellinger PCA(Principal Component Analysis)

##### 장점
> * 빠른 시간에 단어 벡터를 만들 수 있다.
> * 예측 방법에 비해 데이터가 많을 경우 단어 표현이 더 효율적으로 된다.

<br/>

#### 1.2.3. 예측 방법
> 예측 방법이란 신경망 구조나 다른 인공지능 모델을 이용해 특정 문맥에서 어떤 단어가 나올지를 예측하면서 단어를 벡터로 만드는 방식이다.

##### 구현방법
> * Word2Vec **(가장 많이 사용)**
> * NNLM(Neural Network Language Model)
> * RNNLM(Recurrent Neural Network Language Model)

##### Word2Vec
> * CBOW(Continuous Bag of Words) - 여러 단어를 조합해 하나의 단어를 예측하는 방법 (다수 입력 -> 하나 출력)
> * Skip-Gram - 하나의 단어를 가지고 주변에 올 단어를 예측하는 방법 (하나 입력 -> 다수 출력)
> * Skip-Gram이 일반적인 경우 성능이 더 좋아 자주 사용한다.

> **장점:**
> * 카운트 기반 방법보다 단어간 유사도를 더 잘 측정한다.
> * 단어들의 복잡한 특징까지도 잘 잡아낸다.
> * 유사 단어들의 표현이 동일하다. (예: "여자"와 "남자"의 벡터사이 거리와 "엄마"와 "아빠"의 벡터사이 거리가 유사하다.)

<br/><br/>

## 2. 텍스트 분류

<br/>

### 2.1. 개념
> 텍스트 분류는 자연어 처리 문제 중 가장 대표적이고 많이 접하는 부분으로, **특정 텍스트를 인간이 정한 몇 가지 범주 중에 어느 범주에 속하는가를 분류**하는 분야이다.
> * 이중분류(Binary classification): 2가지 범주에 대해 분류
> * 다중분류(Multi class classification): 3가지 이상의 범주에 대해 분류

<br/>

### 2.2. 텍스트 분류 예시
#### 2.2.1. 스팸분류
> * 대표적인 이진분류(일반메일, 스팸메일 분류) 방법

#### 2.2.2. 감정분류
> * 긍정/부정으로 나누면 이진분류지만, 경우에 따라 범주는 "중립"이 올 수 있고, "아주 좋음", "좋음", "보통", "나쁨", "아주 나쁨" 등으로 다양하게 분류를 정할 수 있다.

#### 2.2.3. 기사분류
> * "스포츠", "경제", "국제", "사회" 등 원하는 방법으로 기사의 제목 또는 내용을 분류하는 방법

<br/>

### 2.3. 지도학습을 통한 텍스트 분류
> * 나이브 베이즈 분류(Naive Bayes Classifier)
> * 서포트 벡터 머신(Support Vector Machine, SVM)
> * 신경망(Neural Network, NN)
> * 선형 분류(Linear Classifier)
> * 로지스틱 분류(Logistic Classifier)
> * 랜덤 포레스트(Random Forest)
> * 물론, 여기에 나오는 분류 알고리즘들은 텍스트 분류 이외에도 머신러닝의 모든 분류에 사용된다.

<br/>

### 2.4. 비지도학습을 통한 텍스트 분류
> * k-평균 군집화(k-Means Clustering)
> * 계층적 군집화(Hierarchical Clustering)

<br/><br/>

## 3. 텍스트 유사도

<br/>

### 3.1. 개념
> 

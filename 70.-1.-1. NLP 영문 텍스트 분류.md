# NLP - 영문 텍스트 분류

> NLP에서 영문과 한글을 분리하는 이유는,
> * 영문 데이터가 더 풍부하다.
> * 언어의 특성(띄어쓰기, 붙어있는 조사 등) 한글이 전처리 과정에서 상대적으로 어렵다. 

<br/><br/>

# 1. 개요

<br/>

## 1.1. 목표
> 영화 댓글 데이터를 분석해 "긍정" 및 "부정" 평가 여부 분류

<br/>

## 1.2. 작업 순서
> * ⓐ 데이터 불러오기
> * ⓑ 데이터 분석
> * ⓒ 정제되지 않는 데이터의 전처리
> * ⓓ 문제 해결을 위한 알고리즘 모델링

<br/>

## 1.3. 사용할 데이터
> * 데이터 이름 - Bag of Words Meets Bags of Popcon
> * 데이터 내용 - 캐글 대회의 하나인 "워드 팝콘"에 사용된 데이터로 영화의 리뷰로 구성되 있다.
> * 데이터 용도 - 텍스트 분류 학습을 목적으로 사용
> * 데이터 권한 - MIT (캐글 가입후 사용 권장)
> * 데이터 출처 - https://www.kaggle.com/c/word2vec-nlp-tutorial/data

<br/><br/>

# 2. 데이터 전처리
> [이곳](https://github.com/freemancho1/ai/blob/master/70.-1.-1.-1.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%84%EC%B2%98%EB%A6%AC.md)을 참고하기 바란다.

<br/><br/>

# 3. 인공지능 모델링
> 머신러닝(로지스틱회귀, 랜덤포레스트 모델)과 딥러닝(CNN, RNN)을 이용한 데이터 분류 모델을 소개한다.

<br/>

## 3.1. 머신러닝

<br/>

### 3.1.1. 로지스틱 회귀 모델
> 회귀모델은 주로 이항 분류를 하기 위해 사용되며, 분류 문제에 사용할 수 있는 가장 간단한 모델이다.
> * 로지스틱 회귀 모델: 회귀모델의 결과값에 로지스틱 함수를 적용해 0~1사이의 값을 갖게 하고, 1에 가까우면 1, 0에 가까우면 0을 예측한다.

#### 데이터 로드
```python
import os
import json
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

DATA_PATH = './data/word_popcorn/'
CLEAN_DATA_PATH = DATA_PATH + 'clean/'
OUTPUT_DATA_PATH = DATA_PATH + 'output/'

train_data_df = pd.read_csv(CLEAN_DATA_PATH+'train_data_clean.csv', header=0)
```

#### TF-IDF를 이용한 데이터 전처리
```python
train_review = list(train_data_df['review'])
train_sentiment = list(train_data_df['sentiment'])

# tfidf_vectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True,      #1
#                                    ngram_range=(1,3), max_features=5000)
tfidf_vectorizer = TfidfVectorizer(min_df=0.0, analyzer='word', sublinear_tf=True,      # 이게 더 성능이 좋았음
                                   ngram_range=(1,3), max_features=1000)                # 경우에 따라서겠지만..

train_data = tfidf_vectorizer.fit_transform(train_review)
train_label = np.array(train_sentiment)
```
> #1 부가 설명
> * min_df=0.0 : 특정 토큰의 df(document-frequency, 문서의 표시된 빈도 수)값이 0.0보다 적게 나오면 벡터화 과정에서 제거, 이것은 test 데이터 벡터화 과정에서 train 데이터에 없는 데이터를 삭제하기 위해 사용함.
> * analyzer='char' : 분석하는 기준, 'word'와 'char' 두가지가 있음(나중에 이부분을 word로 수정해 해봐야 할 듯)
> * sublinear_tf=True : 문서의 단어 빈도 수(term frequency)에 대한 스무딩(smoothing, 다듬질) 여부를 설정
> * ngram_range=(1,3) : 각각의 단어(여기서는 char)에 인덱스가 부여되는데, n-gram은 단어의 묶음을 말하며, (1,1)하나의 단어에, (1,2)는 1개 또는 두개의 단어에 인덱스를 부여하라는 의미임, 여기서는 (1,3)이니 1개, 2개, 3개의 단어(여기서는 char)에 인덱스를 부여하라는 의미
> * max_features=5000 : 특정 레코드의 최대 특성(컬럼)수가 5000이 넘으면 제거(5000까지만 유지)

#### 학습용 데이터와 검증용 데이터 분리
```python
RANDOM_SEED = 40          #1
TEST_SPLIT = 0.3          #2

x_train, x_vaild, y_train, y_vaild = \
    train_test_split(train_data, train_label, test_size=TEST_SPLIT, random_state=RANDOM_SEED)
```    
> * #2: 학습 데이터와 검증 데이터를 분류하는 비율 설정
> * #1: 학습 데이터와 검증 데이터를 분류할 때 항상 동일한 데이터로 분류하기 위해 설정(동일성을 유지하기 위해 사용)

#### 모델 지정, 학습 및 검증
```python
model_lr = LogisticRegression(class_weight='balanced')  #1
model_lr.fit(x_train, y_train)

print(f'훈련 정확도: {model_lr.score(x_vaild, y_vaild)}')
## 결과
# 훈련 정확도: 0.8532
```
> * #1: 기본값은 "None"이며, "balanced"는 설명이 넘 어려운데, 좋은말 같으니 그냥 사용하자....

#### 모델 시험 및 결과 저장
```python
test_data_df = pd.read_csv(CLEAN_DATA_PATH+'test_data_clean.csv', header=0)

test_review = list(test_data_df['review'])

test_data = tfidf_vectorizer.fit_transform(test_review)
tfidf_test_predicted = model_lr.predict(test_data)

if not os.path.exists(OUTPUT_DATA_PATH):
    os.makedirs(OUTPUT_DATA_PATH)
    
test_ids = list(test_data_df['id'])
answer_dataset = pd.DataFrame({'id': test_ids, 'sentiment': tfidf_test_predicted})
answer_dataset.set_index('id', inplace=True)
answer_dataset.to_csv(OUTPUT_DATA_PATH+'model_lr_tfidf_answer.csv')
```

#### Word2Vec을 활용한 데이터 전처리
> 각 리뷰를 단어로 구분해 배열에 저장
```python
train_total_words = []
for review in train_review:
    train_total_words.append(review.split())

print(len(train_total_words))
print(train_total_words[0])

## 결과
# 25000
# ['stuff', 'going'... 'hope', 'latter']
```

#### Word2Vec 모델 학습
> Word2Vec은 자체로 하나의 인공지능 모델이기 때문에 별도의 학습을 해줘야 한다.
```python
num_features = 300     # 워드벡터의 특징값 수로, 각 단어에 대한 임베딩된 벡터의 차원 수
min_word_count = 40    # 단어에 대한 최소 빈도 수, 의미있는 빈도수의 단어만 선택하기 위해 적은 빈도 수의 단어는 학습에서 제외
num_workers = 4        # 모델 학습에 사용할 프로세스 개수
context = 10           # 컨텍스트 윈도 크기
downsampling = 1e-3    # 다운 샘플링 비율(빠른 처리를 위해 사용, 보통 0.001 사용)

from gensim.models import word2vec

model_word2vec = word2vec.Word2Vec(
    train_total_words, workers=num_workers, size=num_features, 
    min_count=min_word_count, window=context, sample=downsampling)
```    

#### 단어 패딩(입력 리뷰의 단어 수를 일치시킴, 모델에 들어가는 입력값의 feature수가 동일해야 함)
> 적으면 무효값을 넣고, 많으면 뒤에서 자르는 padding 작업 수행
```python
def get_features(review, model, index2word_set, num_features):
    feature_vector = np.zeros((num_features), dtype=np.float32)        # 출력값 초기화
    
    num_words = 0
    for w in review:
        if w in index2word_set:
            num_words += 1
            feature_vector = np.add(feature_vector, model[w])
    
    feature_vector = np.divide(feature_vector, num_words)
    return feature_vector

def get_dataset(reviews, model, num_features):
    dataset = []
    index2word_set = set(model.wv.index2word)
    
    for review in reviews:
        dataset.append(get_features(review, model, index2word_set, num_features))
        
    reviewFeatureVecs = np.stack(dataset)
    return reviewFeatureVecs
    
wv_train_data = get_dataset(train_total_words, model_word2vec, num_features)
wv_train_label = np.array(train_sentiment)
print(wv_train_data)
print(len(wv_train_data), len(wv_train_data[0]))

## 결과
[[ 0.1330878  -0.04184783 -0.05731816 ... -0.18859217  0.07299545
   0.03740565]
 [ 0.29974112 -0.02441738 -0.11823887 ...  0.00493716 -0.11742931
  -0.1631037 ]
 ...
 [ 0.15234533  0.18643677  0.02934478 ... -0.05553293  0.01507512
   0.19147304]]
25000 300   
```

#### 시험데이터 패딩작업
```python
test_total_words = []
for review in test_review:
    test_total_words.append(review.split())
    
wv_test_data = get_dataset(test_total_words, model_word2vec, num_features)   
```

#### 훈련데이터 분리, 훈련 및 검증
```python
RANDOM_SEED = 40
TEST_SPLIT = 0.3

x_train, x_vaild, y_train, y_vaild = \
    train_test_split(wv_train_data, wv_train_label, 
                     test_size=TEST_SPLIT, random_state=RANDOM_SEED)
                     
model_lr_w2v = LogisticRegression(class_weight='balanced')
model_lr_w2v.fit(x_train, y_train)

print(f'훈련 정확도: {model_lr_w2v.score(x_vaild, y_vaild)}')

## 결과
# 훈련 정확도: 0.8544
```

#### 모델 시험 및 결과 저장
```python
model_lr_w2v_predict = model_lr_w2v.predict(wv_test_data)

answer_dataset = pd.DataFrame({'id': test_ids, 'sentiment': model_lr_w2v_predict},index=None)
answer_dataset.set_index('id', inplace=True)
answer_dataset.to_csv(OUTPUT_DATA_PATH+'model_lr_w2v_answer.csv')
```

<br/>

### 3.1.2. RandomForest
> 여러개의 의사결정 트리(Decision Tree)의 결과값을 평균내어 최종 결과값으로 사용한다. <br/>
> 그나마 머신러닝에서 성능이 좋아 많이 사용한다.

#### 모델 학습, 검증 및 시험
> 위에서 Word2Vec으로 만들어 둔 훈련, 검증 및 시험 데이터를 그대로 사용한다.
```python
from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators=100)
forest.fit(x_train, y_train)

print(f'훈련 정확도: {forest.score(x_vaild, y_vaild)}')

forest_predict = forest.predict(wv_test_data)

answer_dataset = pd.DataFrame({'id': test_ids, 'sentiment': forest_predict},index=None)
answer_dataset.set_index('id', inplace=True)
print(answer_dataset.head())
answer_dataset.to_csv(OUTPUT_DATA_PATH+'forest_w2v_answer.csv')

## 결과
# 훈련 정확도: 0.838 
```
> 성능이 좋은 알고리즘이라고 해도 항상 좋은것은 아니다.

<br/>

## 3.2. Deep learning

<br/>

### 3.2.1. RNN(Recurrent Neural Network, 순환 신경망 모델)
> 순서가 있는 데이터에 대한 처리를 위해 개발된 알고리즘으로, 시스템 로그 분석이나 자연어 처리에서 많이 사용한다.

#### 데이터 불러오기
```python
import os
import json
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from tensorflow.keras.preprocessing.sequence import pad_sequences

DATA_PATH = './data/word_popcorn/'
INPUT_DATA_PATH = DATA_PATH + 'clean/'
OUTPUT_DATA_PATH = DATA_PATH + 'output/'

train_data_median = np.load(open(INPUT_DATA_PATH+'train_data_vec_median.npy', 'rb'))
train_data_max = np.load(open(INPUT_DATA_PATH+'train_data_vec_max.npy', 'rb'))
train_label = np.load(open(INPUT_DATA_PATH+'train_label.npy', 'rb'))
test_data_median = np.load(open(INPUT_DATA_PATH+'test_data_vec_median.npy', 'rb'))
test_data_max = np.load(open(INPUT_DATA_PATH+'test_data_vec_max.npy', 'rb'))

input_config = json.load(open(INPUT_DATA_PATH+'model_config.json', 'r'))
```

#### 랜덤시드 지정 및 하이퍼파라메타 지정
> 하이퍼파라메타 설정을 대부분 **모델 학습을 위한 설정과 모델의 레이어 차원 수 설정**으로 나뉜다.
```python
SEED_NUM = 1234
tf.random.set_seed(SEED_NUM)

model_name = 'rnn_classifier_en'
BATCH_SIZE = 128
NUM_EPOCHS = 5
VALID_SPLIT = 0.1
MAX_LEN_MEDIAN = train_data_median.shape[1]
MAX_LEN_MAX = train_data_max.shape[1]

kargs = {
    'model_name': model_name,
    'vocab_size': input_config['vocab_size'],          #1
    'input_dimension': MAX_LEN_MEDIAN,
    'embedding_dimension': 100,
    'dropout_rate': 0.2,
    'lstm_dimension': 150,
    'dense_dimension': 150,
    'output_dimension': 1
}
```
> * #1: 이 부분이 뒤에서 알고리즘의 입력 차원이 되는데, 이것이 잘 이해되지 않음

#### 모델 정의 및 컴파일
```python
class RNNClassifier(tf.keras.Model):
    
    def __init__(self, **kargs):
        super(RNNClassifier, self).__init__(name=kargs['model_name'])
        self.embedding   = layers.Embedding(input_dim=kargs['vocab_size'], 
                                            output_dim=kargs['embedding_dimension'])
        self.lstm_layer1 = layers.LSTM(kargs['lstm_dimension'], return_sequences=True)
        self.lstm_layer2 = layers.LSTM(kargs['lstm_dimension'])
        self.dropout     = layers.Dropout(kargs['dropout_rate'])
        self.fc1         = layers.Dense(units=kargs['dense_dimension'],
                                        activation=tf.keras.activations.tanh)
        self.fc2         = layers.Dense(units=kargs['output_dimension'],
                                        activation=tf.keras.activations.sigmoid)
        
    def call(self, input):
        model = self.embedding(input)
        model = self.dropout(model)
        model = self.lstm_layer1(model)
        model = self.lstm_layer2(model)
        model = self.dropout(model)
        model = self.fc1(model)
        model = self.dropout(model)
        model = self.fc1(model)           # fc1도 한번만 사용해도 되고, 이렇게 여러번 사용해도 된다.
        model = self.dropout(model)       # 많이 사용하면, 비용이 더 많이 들지만, 정확도가 오른다는 보장은 없다.
        model = self.fc1(model)
        model = self.dropout(model)
        model = self.fc2(model)
        return model

rnn_model = RNNClassifier(**kargs)
rnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
                  loss=tf.keras.losses.BinaryCrossentropy(),
                  metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])
```

#### 다양한 Callback 함수 선언
> 모델을 정의하고 컴파일했으면, 바로 fit함수를 사용해 모델을 학습시킬 수 있지만, <br/>
> 모델 학습중에 발생할 수 있는 다양한 상황을 모니터링하거나 제어하기 위해 callback함수를 사용한다.
```python
SAVE_FILE_NM = 'weights.h5'

earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=1)    #1

checkpoint_path = f'{OUTPUT_DATA_PATH}{model_name}/{SAVE_FILE_NM}'                        #2
checkpoint_dir = os.path.dirname(checkpoint_path)
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir, exist_ok=True)
    
cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, 
                              save_best_only=True, save_weights_only=True)
          
log_dir = './logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')               #3
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)          
```
> #1: EarlyStopping - 오버피팅(Overfiting, 과적합) 현상을 막기위해 사용하며, 특정 에폭에서 검증 평가점수가 이전 에폭의 검증 평가점수보다 낮아지면 모델 학습을 멈추는 역할을 한다.
> * val_accuracy(검증 정확도)가 이전 에폭의 검증 정확도보다 0.0001(1e-4)보다 적어지는 횟수가 1번(patience=1) 이상이면 학습을 종료한다. <br/>

> #2: ModelCheckpoint - 에폭마다 모델을 저장한다.
> * save_best_only는 가장 성능이 좋은 모델을 저장하고, save_weights_only는 그래프 전부가 아닌 모델 가중치만 저장한다. <br/>

> #3: TensorBoard - tensorboard를 보기위해 사용한다.
> * %tensorboard --logdir logs/fit (jupyter lab), %을 빼고 터미널에서 직접 입력해도 된다.

#### 모델 학습 및 결과 그래프
```python
history = rnn_model.fit(train_data_median, train_label,
                        batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,
                        validation_split=VALID_SPLIT, 
                        callbacks=[earlystop_callback, cp_callback, tensorboard_callback])
                        
def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history[f'val_{string}'], '')
    plt.xlabel('Epochs')
    plt.ylabel(string)
    plt.legend([string, f'val_{string}'])
    plt.show()
    
plot_graphs(history, 'accuracy')    
plot_graphs(history, 'loss')
```

#### 모델 시험 및 결과 저장
```python
rnn_model.load_weights(os.path.join(OUTPUT_DATA_PATH, model_name, SAVE_FILE_NM))    # 가장 좋은 모델을 불러온다.

rnn_predict = rnn_model.predict(test_data_median, batch_size=BATCH_SIZE)
rnn_predict = rnn_predict.squeeze(-1)                                               # 2차원 배열을 1차원으로 압축한다.

test_data_df = pd.read_csv(INPUT_DATA_PATH+'test_data_clean.csv', header=0)
output = pd.DataFrame({'id': list(test_data_df['id']), 'sentiment': list(rnn_predict)})
output.set_index('id', inplace=True)
output.to_csv(OUTPUT_DATA_PATH+'rnn_median_result.csv')
```

<br/>

### 3.2.2. CNN(Convolution Neural Network, 합성곱 신경망 모델)
> 이미지 처리에서 시작해 이제는 텍스트 분석 등 다양한 분야에서 활용되고 있는 딥러닝의 핵심 알고리즘이다.

#### 하이퍼파라메타 정의
> 랜덤시드나 데이터를 불러오는 것은 RNN 알고리즘에서 사용했던 것을 그대로 사용한다.
```python
model_name = 'cnn_classifier_en'
BATCH_SIZE = 512
NUM_EPOCHS = 5
VALID_SPLIT = 0.1
MAX_LEN_MEDIAN = train_data_median.shape[1]
MAX_LEN_MAX = train_data_max.shape[1]

kargs = {
    'model_name': model_name,
    'vocab_size': input_config['vocab_size'],
    'input_dimension': MAX_LEN_MEDIAN,
    'embedding_dimension': 128,
    'num_filters': 100,
    'dropout_rate': 0.5,
    'hidden_dimension': 512,
    'output_dimension': 1
}
```

#### 모델정의 및 컴파일
```python
class CNNClassifier(tf.keras.Model):
    
    def __init__(self, **kargs):
        super(CNNClassifier, self).__init__(name=kargs['model_name'])
        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], 
                                          output_dim=kargs['embedding_dimension'])
        self.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, 
                                        padding='valid', activation='relu', 
                                        kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))
                          for kernel_size in [3, 4, 5, 4, 3]]
        self.pooling   = layers.GlobalMaxPool1D()
        self.dropout   = layers.Dropout(kargs['dropout_rate'])
        self.fc1       = layers.Dense(units=kargs['hidden_dimension'], activation='relu',
                                      kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))
        self.fc2       = layers.Dense(units=kargs['output_dimension'], activation='sigmoid',
                                      kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))
        
    def call(self, input):
        model = self.embedding(input)
        model = self.dropout(model)
        model = tf.concat([self.pooling(conv(model)) for conv in self.conv_list], axis=-1)
        model = self.fc1(model)
        model = self.fc2(model)
        return model

cnn_model = CNNClassifier(**kargs)
cnn_model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), 
                  metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])
```

#### Callback 함수 정의
```python
log_dir = f'./logs/fit/{model_name}/{datetime.datetime.now().strftime("%Y%m%d-%H%M%S")}'
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=1)

checkpoint_path = f'{OUTPUT_DATA_PATH}{model_name}/weights.h5'
checkpoint_dir = os.path.dirname(checkpoint_path)
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir, exist_ok=True)
    
cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, 
                              save_best_only=True, save_weights_only=True)
```

#### 모델학습 및 로그보기
```python
history = cnn_model.fit(train_data_median, train_label, 
                        batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,
                        validation_split=VALID_SPLIT, 
                        callbacks=[earlystop_callback, cp_callback, tensorboard_callback])
                        
plot_graphs(history, 'accuracy')
plot_graphs(history, 'loss')
```

#### 모델시험
##### 가장 좋은 모델 불러오기
```python
cnn_model.load_weights(os.path.join(OUTPUT_DATA_PATH, model_name, SAVE_FILE_NM))
```

```python
cnn_predictions = cnn_model.predict(test_data_median, batch_size=BATCH_SIZE)
cnn_predictions = cnn_predictions.squeeze(-1)

output = pd.DataFrame({'id': list(test_data_df['id']), 'sentiment': list(cnn_predictions)})
output.set_index('id', inplace=True)
output.to_csv(OUTPUT_DATA_PATH+'cnn_median_result.csv')
```

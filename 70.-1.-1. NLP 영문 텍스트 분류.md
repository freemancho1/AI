# NLP - 영문 텍스트 분류

> NLP에서 영문과 한글을 분리하는 이유는,
> * 영문 데이터가 더 풍부하다.
> * 언어의 특성(띄어쓰기, 붙어있는 조사 등) 한글이 전처리 과정에서 상대적으로 어렵다. 

<br/><br/>

# 1. 개요

<br/>

## 1.1. 목표
> 영화 댓글 데이터를 분석해 "긍정" 및 "부정" 평가 여부 분류

<br/>

## 1.2. 작업 순서
> * ⓐ 데이터 불러오기
> * ⓑ 데이터 분석
> * ⓒ 정제되지 않는 데이터의 전처리
> * ⓓ 문제 해결을 위한 알고리즘 모델링

<br/>

## 1.3. 사용할 데이터
> * 데이터 이름 - Bag of Words Meets Bags of Popcon
> * 데이터 내용 - 캐글 대회의 하나인 "워드 팝콘"에 사용된 데이터로 영화의 리뷰로 구성되 있다.
> * 데이터 용도 - 텍스트 분류 학습을 목적으로 사용
> * 데이터 권한 - MIT (캐글 가입후 사용 권장)
> * 데이터 출처 - https://www.kaggle.com/c/word2vec-nlp-tutorial/data

<br/><br/>

# 2. 인공지능 모델링
> [이곳](https://github.com/freemancho1/ai/blob/master/70.-1.-1.-1.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%84%EC%B2%98%EB%A6%AC.md)을 참고하기 바란다.

<br/><br/>

# 3. 인공지능 모델링
> 머신러닝(로지스틱회귀, 랜덤포레스트 모델)과 딥러닝(CNN, RNN)을 이용한 데이터 분류 모델을 소개한다.

<br/>

## 3.1. 머신러닝

<br/>

### 3.1.1. 로지스틱 회귀 모델
> 회귀모델은 주로 이항 분류를 하기 위해 사용되며, 분류 문제에 사용할 수 있는 가장 간단한 모델이다.
> * 로지스틱 회귀 모델: 회귀모델의 결과값에 로지스틱 함수를 적용해 0~1사이의 값을 갖게 하고, 1에 가까우면 1, 0에 가까우면 0을 예측한다.

#### 데이터 로드
```python
import os
import json
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

DATA_PATH = './data/word_popcorn/'
CLEAN_DATA_PATH = DATA_PATH + 'clean/'
OUTPUT_DATA_PATH = DATA_PATH + 'output/'

train_data_df = pd.read_csv(CLEAN_DATA_PATH+'train_data_clean.csv', header=0)
```

#### TF-IDF를 이용한 데이터 전처리
```python
train_review = list(train_data_df['review'])
train_sentiment = list(train_data_df['sentiment'])

# tfidf_vectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True,      #1
#                                    ngram_range=(1,3), max_features=5000)
tfidf_vectorizer = TfidfVectorizer(min_df=0.0, analyzer='word', sublinear_tf=True,      # 이게 더 성능이 좋았음
                                   ngram_range=(1,3), max_features=1000)                # 경우에 따라서겠지만..

train_data = tfidf_vectorizer.fit_transform(train_review)
train_label = np.array(train_sentiment)
```
> #1 부가 설명
> * min_df=0.0 : 특정 토큰의 df(document-frequency, 문서의 표시된 빈도 수)값이 0.0보다 적게 나오면 벡터화 과정에서 제거, 이것은 test 데이터 벡터화 과정에서 train 데이터에 없는 데이터를 삭제하기 위해 사용함.
> * analyzer='char' : 분석하는 기준, 'word'와 'char' 두가지가 있음(나중에 이부분을 word로 수정해 해봐야 할 듯)
> * sublinear_tf=True : 문서의 단어 빈도 수(term frequency)에 대한 스무딩(smoothing, 다듬질) 여부를 설정
> * ngram_range=(1,3) : 각각의 단어(여기서는 char)에 인덱스가 부여되는데, n-gram은 단어의 묶음을 말하며, (1,1)하나의 단어에, (1,2)는 1개 또는 두개의 단어에 인덱스를 부여하라는 의미임, 여기서는 (1,3)이니 1개, 2개, 3개의 단어(여기서는 char)에 인덱스를 부여하라는 의미
> * max_features=5000 : 특정 레코드의 최대 특성(컬럼)수가 5000이 넘으면 제거(5000까지만 유지)

#### 학습용 데이터와 검증용 데이터 분리
```python
RANDOM_SEED = 40          #1
TEST_SPLIT = 0.3          #2

x_train, x_vaild, y_train, y_vaild = \
    train_test_split(train_data, train_label, test_size=TEST_SPLIT, random_state=RANDOM_SEED)
```    
> * #2: 학습 데이터와 검증 데이터를 분류하는 비율 설정
> * #1: 학습 데이터와 검증 데이터를 분류할 때 항상 동일한 데이터로 분류하기 위해 설정(동일성을 유지하기 위해 사용)

#### 모델 지정, 학습 및 검증
```python
model_lr = LogisticRegression(class_weight='balanced')  #1
model_lr.fit(x_train, y_train)

print(f'훈련 정확도: {model_lr.score(x_vaild, y_vaild)}')
## 결과
# 훈련 정확도: 0.8532
```
> * #1: 기본값은 "None"이며, "balanced"는 설명이 넘 어려운데, 좋은말 같으니 그냥 사용하자....

#### 모델 시험 및 결과 저장
```python
test_data_df = pd.read_csv(CLEAN_DATA_PATH+'test_data_clean.csv', header=0)

test_review = list(test_data_df['review'])

test_data = tfidf_vectorizer.fit_transform(test_review)
tfidf_test_predicted = model_lr.predict(test_data)

if not os.path.exists(OUTPUT_DATA_PATH):
    os.makedirs(OUTPUT_DATA_PATH)
    
test_ids = list(test_data_df['id'])
answer_dataset = pd.DataFrame({'id': test_ids, 'sentiment': tfidf_test_predicted})
answer_dataset.set_index('id', inplace=True)
answer_dataset.to_csv(OUTPUT_DATA_PATH+'model_lr_tfidf_answer.csv')
```

#### Word2Vec을 활용한 데이터 전처리
> 각 리뷰를 단어로 구분해 배열에 저장
```python
train_total_words = []
for review in train_review:
    train_total_words.append(review.split())

print(len(train_total_words))
print(train_total_words[0])

## 결과
# 25000
# ['stuff', 'going'... 'hope', 'latter']
```

#### Word2Vec 모델 학습
> Word2Vec은 자체로 하나의 인공지능 모델이기 때문에 별도의 학습을 해줘야 한다.
```python
num_features = 300     # 워드벡터의 특징값 수로, 각 단어에 대한 임베딩된 벡터의 차원 수
min_word_count = 40    # 단어에 대한 최소 빈도 수, 의미있는 빈도수의 단어만 선택하기 위해 적은 빈도 수의 단어는 학습에서 제외
num_workers = 4        # 모델 학습에 사용할 프로세스 개수
context = 10           # 컨텍스트 윈도 크기
downsampling = 1e-3    # 다운 샘플링 비율(빠른 처리를 위해 사용, 보통 0.001 사용)

from gensim.models import word2vec

model_word2vec = word2vec.Word2Vec(
    train_total_words, workers=num_workers, size=num_features, 
    min_count=min_word_count, window=context, sample=downsampling)
```    

#### 단어 패딩(입력 리뷰의 단어 수를 일치시킴, 모델에 들어가는 입력값의 feature수가 동일해야 함)
> 적으면 무효값을 넣고, 많으면 뒤에서 자르는 padding 작업 수행
```python
def get_features(review, model, index2word_set, num_features):
    feature_vector = np.zeros((num_features), dtype=np.float32)        # 출력값 초기화
    
    num_words = 0
    for w in review:
        if w in index2word_set:
            num_words += 1
            feature_vector = np.add(feature_vector, model[w])
    
    feature_vector = np.divide(feature_vector, num_words)
    return feature_vector

def get_dataset(reviews, model, num_features):
    dataset = []
    index2word_set = set(model.wv.index2word)
    
    for review in reviews:
        dataset.append(get_features(review, model, index2word_set, num_features))
        
    reviewFeatureVecs = np.stack(dataset)
    return reviewFeatureVecs
    
wv_train_data = get_dataset(train_total_words, model_word2vec, num_features)
wv_train_label = np.array(train_sentiment)
print(wv_train_data)
print(len(wv_train_data), len(wv_train_data[0]))

## 결과
[[ 0.1330878  -0.04184783 -0.05731816 ... -0.18859217  0.07299545
   0.03740565]
 [ 0.29974112 -0.02441738 -0.11823887 ...  0.00493716 -0.11742931
  -0.1631037 ]
 ...
 [ 0.15234533  0.18643677  0.02934478 ... -0.05553293  0.01507512
   0.19147304]]
25000 300   
```

#### 시험데이터 패딩작업
```python
test_total_words = []
for review in test_review:
    test_total_words.append(review.split())
    
wv_test_data = get_dataset(test_total_words, model_word2vec, num_features)   
```

#### 훈련데이터 분리, 훈련 및 검증
```python
RANDOM_SEED = 40
TEST_SPLIT = 0.3

x_train, x_vaild, y_train, y_vaild = \
    train_test_split(wv_train_data, wv_train_label, 
                     test_size=TEST_SPLIT, random_state=RANDOM_SEED)
                     
model_lr_w2v = LogisticRegression(class_weight='balanced')
model_lr_w2v.fit(x_train, y_train)

print(f'훈련 정확도: {model_lr_w2v.score(x_vaild, y_vaild)}')

## 결과
# 훈련 정확도: 0.8544
```

#### 모델 시험 및 결과 저장
```python
model_lr_w2v_predict = model_lr_w2v.predict(wv_test_data)

answer_dataset = pd.DataFrame({'id': test_ids, 'sentiment': model_lr_w2v_predict},index=None)
answer_dataset.set_index('id', inplace=True)
answer_dataset.to_csv(OUTPUT_DATA_PATH+'model_lr_w2v_answer.csv')
```

<br/>

### 3.1.2. RandomForest
> 여러개의 의사결정 트리(Decision Tree)의 결과값을 평균내어 최종 결과값으로 사용한다. <br/>
> 그나마 머신러닝에서 성능이 좋아 많이 사용한다.

#### 모델 학습, 검증 및 시험
> 위에서 Word2Vec으로 만들어 둔 훈련, 검증 및 시험 데이터를 그대로 사용한다.
```python
from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators=100)
forest.fit(x_train, y_train)

print(f'훈련 정확도: {forest.score(x_vaild, y_vaild)}')

forest_predict = forest.predict(wv_test_data)

answer_dataset = pd.DataFrame({'id': test_ids, 'sentiment': forest_predict},index=None)
answer_dataset.set_index('id', inplace=True)
print(answer_dataset.head())
answer_dataset.to_csv(OUTPUT_DATA_PATH+'forest_w2v_answer.csv')

## 결과
# 훈련 정확도: 0.838 
```
> 성능이 좋은 알고리즘이라고 해도 항상 좋은것은 아니다.


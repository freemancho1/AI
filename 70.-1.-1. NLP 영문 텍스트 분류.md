# NLP - 영문 텍스트 분류

> NLP에서 영문과 한글을 분리하는 이유는,
> * 영문 데이터가 더 풍부하다.
> * 언어의 특성(띄어쓰기, 붙어있는 조사 등) 한글이 전처리 과정에서 상대적으로 어렵다. 

<br/><br/>

# 1. 개요

<br/>

## 1.1. 목표
> 영화 댓글 데이터를 분석해 "긍정" 및 "부정" 평가 여부 분류

<br/>

## 1.2. 작업 순서
> * ⓐ 데이터 불러오기
> * ⓑ 데이터 분석
> * ⓒ 정제되지 않는 데이터의 전처리
> * ⓓ 문제 해결을 위한 알고리즘 모델링

<br/>

## 1.3. 사용할 데이터
> * 데이터 이름 - Bag of Words Meets Bags of Popcon
> * 데이터 내용 - 캐글 대회의 하나인 "워드 팝콘"에 사용된 데이터로 영화의 리뷰로 구성되 있다.
> * 데이터 용도 - 텍스트 분류 학습을 목적으로 사용
> * 데이터 권한 - MIT (캐글 가입후 사용 권장)
> * 데이터 출처 - https://www.kaggle.com/c/word2vec-nlp-tutorial/data

<br/><br/>

# 2. 인공지능 모델링
> [이곳](https://github.com/freemancho1/ai/blob/master/70.-1.-1.-1.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%84%EC%B2%98%EB%A6%AC.md)을 참고하기 바란다.

<br/><br/>

# 3. 인공지능 모델링
> 머신러닝(로지스틱회귀, 랜덤포레스트 모델)과 딥러닝(CNN, RNN)을 이용한 데이터 분류 모델을 소개한다.

<br/>

## 3.1. 머신러닝

<br/>

### 3.1.1. 로지스틱 회귀 모델
> 회귀모델은 주로 이항 분류를 하기 위해 사용되며, 분류 문제에 사용할 수 있는 가장 간단한 모델이다.
> * 로지스틱 회귀 모델: 회귀모델의 결과값에 로지스틱 함수를 적용해 0~1사이의 값을 갖게 하고, 1에 가까우면 1, 0에 가까우면 0을 예측한다.

#### 데이터 로드
```python
import os
import json
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

DATA_PATH = './data/word_popcorn/'
CLEAN_DATA_PATH = DATA_PATH + 'clean/'
OUTPUT_DATA_PATH = DATA_PATH + 'output/'

train_data_df = pd.read_csv(CLEAN_DATA_PATH+'train_data_clean.csv', header=0)
```

#### TF-IDF를 이용한 데이터 전처리
```python
train_review = list(train_data_df['review'])
train_sentiment = list(train_data_df['sentiment'])

tfidf_vectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True,      #1
                                   ngram_range=(1,3), max_features=5000)

train_data = tfidf_vectorizer.fit_transform(train_review)
train_label = np.array(train_sentiment)
```
> #1 부가 설명
> * min_df=0.0 : 특정 토큰의 df(document-frequency, 문서의 표시된 빈도 수)값이 0.0보다 적게 나오면 벡터화 과정에서 제거, 이것은 test 데이터 벡터화 과정에서 train 데이터에 없는 데이터를 삭제하기 위해 사용함.
> * analyzer='char' : 분석하는 기준, 'word'와 'char' 두가지가 있음(나중에 이부분을 word로 수정해 해봐야 할 듯)
> * sublinear_tf=True : 문서의 단어 빈도 수(term frequency)에 대한 스무딩(smoothing, 다듬질) 여부를 설정
> * ngram_range=(1,3) : 각각의 단어(여기서는 char)에 인덱스가 부여되는데, n-gram은 단어의 묶음을 말하며, (1,1)하나의 단어에, (1,2)는 1개 또는 두개의 단어에 인덱스를 부여하라는 의미임, 여기서는 (1,3)이니 1개, 2개, 3개의 단어(여기서는 char)에 인덱스를 부여하라는 의미
> * max_features=5000 : 벡터의 최대 길이 설정

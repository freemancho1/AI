# NLP - 영문 텍스트 분류

> NLP에서 영문과 한글을 분리하는 이유는,
> * 영문 데이터가 더 풍부하다.
> * 언어의 특성(띄어쓰기, 붙어있는 조사 등) 한글이 전처리 과정에서 상대적으로 어렵다. 

<br/><br/>

# 1. 개요

<br/>

## 1.1. 목표
> 영화 댓글 데이터를 분석해 "긍정" 및 "부정" 평가 여부 분류

<br/>

## 1.2. 작업 순서
> * ⓐ 데이터 불러오기
> * ⓑ 데이터 분석
> * ⓒ 정제되지 않는 데이터의 전처리
> * ⓓ 문제 해결을 위한 알고리즘 모델링

<br/>

## 1.3. 사용할 데이터
> * 데이터 이름 - Bag of Words Meets Bags of Popcon
> * 데이터 내용 - 캐글 대회의 하나인 "워드 팝콘"에 사용된 데이터로 영화의 리뷰로 구성되 있다.
> * 데이터 용도 - 텍스트 분류 학습을 목적으로 사용
> * 데이터 권한 - MIT (캐글 가입후 사용 권장)
> * 데이터 출처 - https://www.kaggle.com/c/word2vec-nlp-tutorial/data

<br/><br/>

# 2. 인공지능 모델링

<br/>

## 2.1. 데이터 다운로드

<br/>

### 2.1.1. kaggle 라이브러리 이용
```python
$ kaggle cometitions download -c word2vec-nlp-tutorial
```
> 대회명이 틀렸나 403에러 발생 <br/>
> 정상적으로 다운되면 4개의 파일이 생성된다.

<br/>

### 2.1.2. 직접 홈페이지에서 다운받기
> 홈페이지에 접속해 대회명을 입력해 다운받을 수 있다.

<br/>

## 2.2. 데이터 불러오기

<br/>

### 2.2.1. 압축 해제
```python
import zipfile

DATA_IN_PATH = '../DATA/word_popcorn/'
file_list = ['labeledTrainData.tsv.zip', 'unlabeledTrainData.tsv.zip', 'testData.tsv.zip']

for file in file_list:
    zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')
    zipRef.extractall(DATA_IN_PATH)
    zipRef.close()
```

<br/>

### 2.2.2. pandas로 읽기
> 파일을 판다스로 읽기 전에 먼저 간단히 파일이 어떻게 생겼는지 확인한다.
```python
$ head labeledTrainData.tsv
```
> header가 있는것을 알 수 있고, 당연히 tsv니 구분자는 tab임을 알 수 있으며, 값들이 큰 따옴표로 엮어 있음을 알 수 있다.

<br/>

```python
import numpy as np
import pandas as pd
import os
import csv
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

train_df = pd.read_csv(DATA_IN_PATH+'labeledTrainData.tsv', header=0, delimiter='\t', quoting=csv.QUOTE_ALL)
# header=0: 첫 번째 줄은 header로 사용한다. 기본값은 infer(추론), header=None는 첫 번째 줄부터 바로 데이터로 활용
# delimiter='\t': 'sep='과 동일함
# quoting=2: 처음과 끝의 큰 따옴표는 무시
# 자세한 내용은 아래 참조
# print(pd.read_csv.__doc__)
```
<img src="https://user-images.githubusercontent.com/31339365/102861593-45541c00-4473-11eb-9be7-11cd8ffb28ec.png"></img>

<br/>

## 2.3. 데이터 분석

<br/>

### 2.3.1. pandas_profiling으로 분석
```python
import pandas_profiling

train_df_pp = train_df.profile_report()
train_df_pp.to_file('./train_df_pp.html')
```
##### 결과
<img src="https://user-images.githubusercontent.com/31339365/102862211-15594880-4474-11eb-89f8-fb6aa3014f18.png"></img>
> 각 컬럼의 정보, 처음과 끝 데이터 샘플 등 다양한 정보를 볼 수 있다.

<br/>

### 2.3.2. 기본적인 함수로 분석
> 여기서는 numpy를 이용해 pandas 데이터를 다루는 다양한 방법을 확인한다.

#### 앞/뒤 데이터 보기
```python
train_df.head()
train_df.tail()
```

#### 파일의 크기 확인
```python
import os

print('파일 크기:')
for file in os.listdir(DATA_IN_PATH):
    if 'tsv' in file:
        print(f'{file.ljust(30)} {str(round(os.path.getsize(DATA_IN_PATH+file)/(1024*1024), 2))} MBytes')

## 결과
# 파일 크기:
# labeledTrainData.tsv           32.0 MBytes
# testData.tsv                   31.21 MBytes
# unlabeledTrainData.tsv         64.16 MBytes
```

#### 학습 데이터 크기 및 각 댓글의 길이 확인
```python
print(f'전체 학습 데이터의 개수: {len(train_df)}')

# 각 댓글의 길이 저장
train_review_length = train_df['review'].apply(len)
train_review_length.head()

## 결과
# 전체 학습 데이터의 개수: 25000
# 0    2304
# 1     948
# 2    2451
# 3    2247
# 4    2233
# Name: review, dtype: int64
```

#### 리뷰 길이 분석
##### 이미지로 분석
```python
plt.figure(figsize=(12,5))
plt.hist(train_review_length, bins=200, alpha=0.3, color='r', label='word')
plt.yscale('log', nonpositive='clip')
plt.title('Log-Histogram of length of review')
plt.xlabel('Length of review')
plt.ylabel('Number of review')
```
<img src="https://user-images.githubusercontent.com/31339365/102867067-8d773c80-447b-11eb-9425-07b0ccec3336.png"></img>

##### 최대/평균/최대 길이 확인
```python
print(f'리뷰 길이 최소값: {np.min(train_review_length)}')
print(f'리뷰 길이 평균값: {np.mean(train_review_length)}')
print(f'리뷰 길이 최대값: {np.max(train_review_length)}')
print(f'리뷰 길이 중앙값: {np.median(train_review_length)}')

## 결과
# 리뷰 길이 최소값: 54
# 리뷰 길이 평균값: 1329.71056
# 리뷰 길이 최대값: 13710
# 리뷰 길이 중앙값: 983.0
```

#### 리뷰 단어 갯수 분석
```python
train_word_counts = train_df['review'].apply(lambda x: len(x.split(' ')))

print(f'리뷰 단어 갯수 최소값: {np.min(train_word_counts)}')
print(f'리뷰 단어 갯수 평균값: {np.mean(train_word_counts)}')
print(f'리뷰 단어 갯수 중앙값: {np.median(train_word_counts)}')
print(f'리뷰 단어 갯수 최대값: {np.max(train_word_counts)}')

## 결과
# 리뷰 단어 갯수 최소값: 10
# 리뷰 단어 갯수 평균값: 233.78624
# 리뷰 단어 갯수 중앙값: 174.0
# 리뷰 단어 갯수 최대값: 2470
```

#### WordCloud
```python
from wordcloud import WordCloud

train_df_cloud = WordCloud(width=1000, height=600).generate(' '.join(train_df['review']))
plt.figure(figsize=(20, 12))
plt.imshow(train_df_cloud)
plt.axis('off')
```
<img src="https://user-images.githubusercontent.com/31339365/102867817-a9c7a900-447c-11eb-9276-0a9a5456cf6e.png"></img>
> 이미지를 보면 "br"이 많이 있는것을 볼 수 있는데, 이는 HTML 태그로 좀 있음 제거된다.

#### 라벨 분포 확인
```python
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(6,3)
sns.countplot(x=train_df['sentiment'])
```
<img src="https://user-images.githubusercontent.com/31339365/102870728-c960d080-4480-11eb-8170-8bd908ac8e20.png"></img>
```python
print(f'긍정 리뷰 갯수: {train_df["sentiment"].value_counts()[1]}')
print(f'부정 리뷰 갯수: {train_df["sentiment"].value_counts()[0]}')

## 결과
# 긍정 리뷰 갯수: 12500
# 부정 리뷰 갯수: 12500
```

#### 물음표/마침표 여부, 첫글자 대문자 여부, 숫자가 있는지 여부 등 확인
> 다양한 형태의 pandas 데이터 처리 방법 확인
```python
qmark = np.mean(train_df['review'].apply(lambda x: x[-1]=='?'))
qmark2 = np.mean(train_df['review'].apply(lambda x: '?' in x))
period = np.mean(train_df['review'].apply(lambda x: x[-1]=='.'))
first_capital = np.mean(train_df['review'].apply(lambda x: x[0].isupper()))
capitals = np.mean(train_df['review'].apply(lambda x: max([y.isupper() for y in x])))
digits = np.mean(train_df['review'].apply(lambda x: max([y.isdigit() for y in x])))
digits_re = np.mean(train_df['review'].apply(lambda x: 1 if re.search('\d+', x) is not None else 0))

print(f'{"물음표로 끝나는 질문".ljust(30)} \t: {qmark*100}')
print(f'{"물음표가 있는 질문".ljust(30)} \t: {qmark2*100}')
print(f'{"마침표로 끝나는 질문".ljust(30)} \t: {period*100}')
print(f'{"첫 글자가 대문자인 질문".ljust(30)} \t: {first_capital*100}')
print(f'{"대문자가 있는 질문".ljust(30)} \t: {capitals*100}')
print(f'{"숫자가 있는 질문".ljust(30)} \t: {digits*100}')
print(f'{"숫자가 있는 질문2".ljust(30)} \t: {digits_re*100}')

## 결과
# 물음표로 끝나는 질문                     : 1.9
# 물음표가 있는 질문                       : 29.552
# 마침표로 끝나는 질문                     : 72.048
# 첫 글자가 대문자인 질문                   : 92.84400000000001
# 대문자가 있는 질문                       : 99.592
# 숫자가 있는 질문                        : 56.65599999999999
# 숫자가 있는 질문2                       : 56.65599999999999
```

<br/>

## 2.4. 데이터 전처리

<br/>

### 2.4.1. 주로 사용하는 라이브러리들
> * 데이터 가공: numpy, pandas, json
> * 데이터 정제: re, bs4(Beautiful Soup)
> * 불용어 제거: NLTK의 stopwords
> * ??? : tensorflow의 pad_sequences, Tokenizer

```python
import re
import pandas as pd
import numpy as np
import json
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
```

<br/>

### 2.4.2. 데이터 전처리 방향 결정을 위한 데이터 세부 확인(분석단계와 유사)
```python
print(train_df['review'][0])
```
> With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive bu... <br/>




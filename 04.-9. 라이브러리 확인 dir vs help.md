# 라이브러리가 궁금할때?
> 파이썬을 이용해 프로젝트를 진행하다 보면 다양한 라이브러리를 접하게 되는데, 이때 모든 라이브러리의 하부 패키지나 라이브러리 자체의 인자값이 궁금할 때가 많다. <br/>
> 이를 위해 파이썬에서 자체적으로 제공하는 'dir()'과 'help()' 함수를 소개한다. <br/>
> 몰라서 그렇치 사용법은 간단하다.

<br/><br/>

## 안내 책자(Directory): dir()
> 말 그대로 해당 라이브러리의 '안내 책자'이긴 한데, 자세한 내용 보다는 해당 라이브러리의 하위 패키지와 제공하는 속성정보 등을 간단하게 표시해 준다.

```python
print(dir(tf.keras.layers.LSTM))

## 결과
['_TF_MODULE_IGNORED_PROPERTIES',
 '__call__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getstate__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__setstate__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_add_trackable',
 '_add_variable_with_custom_getter',
 '_autographed_call',
 '_call_accepts_kwargs',
 '_call_arg_was_passed',
 '_call_fn_arg_defaults',
 '_call_fn_arg_positions',
 '_call_fn_args',
 '_call_full_argspec',
 '_cast_single_input',
 '_checkpoint_dependencies',
 '_clear_losses',
 '_compute_dtype',
 '_create_dropout_mask',
 '_create_non_trackable_mask_cache',
 '_create_recurrent_dropout_mask',
 '_dedup_weights',
 '_deferred_dependencies',
 '_dtype',
 '_eager_losses',
 '_flatten',
 '_flatten_layers',
 '_functional_construction_call',
 '_gather_children_attribute',
 '_gather_saveables_for_checkpoint',
 '_get_call_arg_value',
 '_get_existing_metric',
 '_get_input_masks',
 '_get_node_attribute_at_index',
 '_get_save_spec',
 '_get_trainable_state',
 '_handle_activity_regularization',
 '_handle_deferred_dependencies',
 '_handle_weight_regularization',
 '_inbound_nodes',
 '_infer_output_signature',
 '_init_call_fn_args',
 '_init_set_name',
 '_instrument_layer_creation',
 '_is_layer',
 '_keras_api_names',
 '_keras_api_names_v1',
 '_keras_tensor_symbolic_call',
 '_list_extra_dependencies_for_serialization',
 '_list_functions_for_serialization',
 '_lookup_dependency',
 '_map_resources',
 '_maybe_build',
 '_maybe_cast_inputs',
 '_maybe_create_attribute',
 '_maybe_initialize_trackable',
 '_maybe_reset_cell_dropout_mask',
 '_must_restore_from_config',
 '_name_based_attribute_restore',
 '_name_based_restores',
 '_name_scope',
 '_no_dependency',
 '_obj_reference_counts',
 '_object_identifier',
 '_outbound_nodes',
 '_preload_simple_restoration',
 '_process_inputs',
 '_restore_from_checkpoint_position',
 '_set_call_arg_value',
 '_set_connectivity_metadata',
 '_set_dtype_policy',
 '_set_mask_keras_history_checked',
 '_set_mask_metadata',
 '_set_save_spec',
 '_set_trainable_state',
 '_set_training_mode',
 '_setattr_tracking',
 '_should_cast_single_input',
 '_single_restoration_from_checkpoint_position',
 '_split_out_first_arg',
 '_symbolic_call',
 '_tf_api_names',
 '_tf_api_names_v1',
 '_tf_docs_do_not_document',
 '_track_trackable',
 '_trackable_saved_model_saver',
 '_tracking_metadata',
 '_unconditional_checkpoint_dependencies',
 '_unconditional_dependency_names',
 '_update_uid',
 '_validate_args_if_ragged',
 '_validate_state_spec',
 'activation',
 'activity_regularizer',
 'add_loss',
 'add_metric',
 'add_update',
 'add_variable',
 'add_weight',
 'apply',
 'bias_constraint',
 'bias_initializer',
 'bias_regularizer',
 'build',
 'call',
 'compute_dtype',
 'compute_mask',
 'compute_output_shape',
 'compute_output_signature',
 'count_params',
 'dropout',
 'dtype',
 'dtype_policy',
 'dynamic',
 'from_config',
 'get_config',
 'get_dropout_mask_for_cell',
 'get_initial_state',
 'get_input_at',
 'get_input_mask_at',
 'get_input_shape_at',
 'get_losses_for',
 'get_output_at',
 'get_output_mask_at',
 'get_output_shape_at',
 'get_recurrent_dropout_mask_for_cell',
 'get_updates_for',
 'get_weights',
 'implementation',
 'inbound_nodes',
 'input',
 'input_mask',
 'input_shape',
 'input_spec',
 'kernel_constraint',
 'kernel_initializer',
 'kernel_regularizer',
 'losses',
 'metrics',
 'name',
 'name_scope',
 'non_trainable_variables',
 'non_trainable_weights',
 'outbound_nodes',
 'output',
 'output_mask',
 'output_shape',
 'recurrent_activation',
 'recurrent_constraint',
 'recurrent_dropout',
 'recurrent_initializer',
 'recurrent_regularizer',
 'reset_dropout_mask',
 'reset_recurrent_dropout_mask',
 'reset_states',
 'set_weights',
 'stateful',
 'states',
 'submodules',
 'supports_masking',
 'trainable',
 'trainable_variables',
 'trainable_weights',
 'unit_forget_bias',
 'units',
 'updates',
 'use_bias',
 'variable_dtype',
 'variables',
 'weights',
 'with_name_scope']
```
> 

<br/><br/>

## 도움말: help()
> 해당 라이브러리에 대한 자세한 설명이 들어있다. <br/>
> 초기화함수의 인자, 사용방법 등등이 있으며, 자세한 내용은 아래를 참고하기 바란다.

```python
print(help(tf.keras.layers.LSTM))

## 결과
Help on class LSTM in module tensorflow.python.keras.layers.recurrent_v2:

class LSTM(tensorflow.python.keras.layers.recurrent.DropoutRNNCellMixin, tensorflow.python.keras.layers.recurrent.LSTM)
 |  LSTM(*args, **kwargs)
 |  
 |  Long Short-Term Memory layer - Hochreiter 1997.
 |  
 |  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)
 |  for details about the usage of RNN API.
 |  
 |  Based on available runtime hardware and constraints, this layer
 |  will choose different implementations (cuDNN-based or pure-TensorFlow)
 |  to maximize the performance. If a GPU is available and all
 |  the arguments to the layer meet the requirement of the CuDNN kernel
 |  (see below for details), the layer will use a fast cuDNN implementation.
 |  
 |  The requirements to use the cuDNN implementation are:
 |  
 |  1. `activation` == `tanh`
 |  2. `recurrent_activation` == `sigmoid`
 |  3. `recurrent_dropout` == 0
 |  4. `unroll` is `False`
 |  5. `use_bias` is `True`
 |  6. Inputs, if use masking, are strictly right-padded.
 |  7. Eager execution is enabled in the outermost context.
 |  
 |  For example:
 |  
 |  >>> inputs = tf.random.normal([32, 10, 8])
 |  >>> lstm = tf.keras.layers.LSTM(4)
 |  >>> output = lstm(inputs)
 |  >>> print(output.shape)
 |  (32, 4)
 |  >>> lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)
 |  >>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
 |  >>> print(whole_seq_output.shape)
 |  (32, 10, 4)
 |  >>> print(final_memory_state.shape)
 |  (32, 4)
 |  >>> print(final_carry_state.shape)
 |  (32, 4)
 |  
 |  Arguments:
 |    units: Positive integer, dimensionality of the output space.
 |    activation: Activation function to use.
 |      Default: hyperbolic tangent (`tanh`). If you pass `None`, no activation
 |      is applied (ie. "linear" activation: `a(x) = x`).
 |    recurrent_activation: Activation function to use for the recurrent step.
 |      Default: sigmoid (`sigmoid`). If you pass `None`, no activation is
 |      applied (ie. "linear" activation: `a(x) = x`).
 |    use_bias: Boolean (default `True`), whether the layer uses a bias vector.
 |    kernel_initializer: Initializer for the `kernel` weights matrix, used for
 |      the linear transformation of the inputs. Default: `glorot_uniform`.
 |    recurrent_initializer: Initializer for the `recurrent_kernel` weights
 |      matrix, used for the linear transformation of the recurrent state.
 |      Default: `orthogonal`.
 |    bias_initializer: Initializer for the bias vector. Default: `zeros`.
 |    unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of
 |      the forget gate at initialization. Setting it to true will also force
 |      `bias_initializer="zeros"`. This is recommended in [Jozefowicz et
 |          al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).
 |    kernel_regularizer: Regularizer function applied to the `kernel` weights
 |      matrix. Default: `None`.
 |    recurrent_regularizer: Regularizer function applied to the
 |      `recurrent_kernel` weights matrix. Default: `None`.
 |    bias_regularizer: Regularizer function applied to the bias vector. Default:
 |      `None`.
 |    activity_regularizer: Regularizer function applied to the output of the
 |      layer (its "activation"). Default: `None`.
 |    kernel_constraint: Constraint function applied to the `kernel` weights
 |      matrix. Default: `None`.
 |    recurrent_constraint: Constraint function applied to the `recurrent_kernel`
 |      weights matrix. Default: `None`.
 |    bias_constraint: Constraint function applied to the bias vector. Default:
 |      `None`.
 |    dropout: Float between 0 and 1. Fraction of the units to drop for the linear
 |      transformation of the inputs. Default: 0.
 |    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for
 |      the linear transformation of the recurrent state. Default: 0.
 |    return_sequences: Boolean. Whether to return the last output. in the output
 |      sequence, or the full sequence. Default: `False`.
 |    return_state: Boolean. Whether to return the last state in addition to the
 |      output. Default: `False`.
 |    go_backwards: Boolean (default `False`). If True, process the input sequence
 |      backwards and return the reversed sequence.
 |    stateful: Boolean (default `False`). If True, the last state for each sample
 |      at index i in a batch will be used as initial state for the sample of
 |      index i in the following batch.
 |    time_major: The shape format of the `inputs` and `outputs` tensors.
 |      If True, the inputs and outputs will be in shape
 |      `[timesteps, batch, feature]`, whereas in the False case, it will be
 |      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more
 |      efficient because it avoids transposes at the beginning and end of the
 |      RNN calculation. However, most TensorFlow data is batch-major, so by
 |      default this function accepts input and emits output in batch-major
 |      form.
 |    unroll: Boolean (default `False`). If True, the network will be unrolled,
 |      else a symbolic loop will be used. Unrolling can speed-up a RNN, although
 |      it tends to be more memory-intensive. Unrolling is only suitable for short
 |      sequences.
 |  
 |  Call arguments:
 |    inputs: A 3D tensor with shape `[batch, timesteps, feature]`.
 |    mask: Binary tensor of shape `[batch, timesteps]` indicating whether
 |      a given timestep should be masked (optional, defaults to `None`).
 |    training: Python boolean indicating whether the layer should behave in
 |      training mode or in inference mode. This argument is passed to the cell
 |      when calling it. This is only relevant if `dropout` or
 |      `recurrent_dropout` is used (optional, defaults to `None`).
 |    initial_state: List of initial state tensors to be passed to the first
 |      call of the cell (optional, defaults to `None` which causes creation
 |      of zero-filled initial state tensors).
 |  
 |  Method resolution order:
 |      LSTM
 |      tensorflow.python.keras.layers.recurrent.DropoutRNNCellMixin
 |      tensorflow.python.keras.layers.recurrent.LSTM
 |      tensorflow.python.keras.layers.recurrent.RNN
 |      tensorflow.python.keras.engine.base_layer.Layer
 |      tensorflow.python.module.module.Module
 |      tensorflow.python.training.tracking.tracking.AutoTrackable
 |      tensorflow.python.training.tracking.base.Trackable
 |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __init__(self, units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, time_major=False, unroll=False, **kwargs)
 |  
 |  call(self, inputs, mask=None, training=None, initial_state=None)
 |      This is where the layer's logic lives.
 |      
 |      Note here that `call()` method in `tf.keras` is little bit different
 |      from `keras` API. In `keras` API, you can pass support masking for
 |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`
 |      method to support masking.
 |      
 |      Arguments:
 |          inputs: Input tensor, or list/tuple of input tensors.
 |          **kwargs: Additional keyword arguments. Currently unused.
 |      
 |      Returns:
 |          A tensor or list/tuple of tensors.
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from tensorflow.python.keras.layers.recurrent.DropoutRNNCellMixin:
 |  
 |  __getstate__(self)
 |  
 |  __setstate__(self, state)
 |  
 |  get_dropout_mask_for_cell(self, inputs, training, count=1)
 |      Get the dropout mask for RNN cell's input.
 |      
 |      It will create mask based on context if there isn't any existing cached
 |      mask. If a new mask is generated, it will update the cache in the cell.
 |      
 |      Args:
 |        inputs: The input tensor whose shape will be used to generate dropout
 |          mask.
 |        training: Boolean tensor, whether its in training mode, dropout will be
 |          ignored in non-training mode.
 |        count: Int, how many dropout mask will be generated. It is useful for cell
 |          that has internal weights fused together.
 |      Returns:
 |        List of mask tensor, generated or cached mask based on context.
 |  
 |  get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1)
 |      Get the recurrent dropout mask for RNN cell.
 |      
 |      It will create mask based on context if there isn't any existing cached
 |      mask. If a new mask is generated, it will update the cache in the cell.
 |      
 |      Args:
 |        inputs: The input tensor whose shape will be used to generate dropout
 |          mask.
 |        training: Boolean tensor, whether its in training mode, dropout will be
 |          ignored in non-training mode.
 |        count: Int, how many dropout mask will be generated. It is useful for cell
 |          that has internal weights fused together.
 |      Returns:
 |        List of mask tensor, generated or cached mask based on context.
 |  
 |  reset_dropout_mask(self)
 |      Reset the cached dropout masks if any.
 |      
 |      This is important for the RNN layer to invoke this in it `call()` method so
 |      that the cached mask is cleared before calling the `cell.call()`. The mask
 |      should be cached across the timestep within the same batch, but shouldn't
 |      be cached between batches. Otherwise it will introduce unreasonable bias
 |      against certain index of data within the batch.
 |  
 |  reset_recurrent_dropout_mask(self)
 |      Reset the cached recurrent dropout masks if any.
 |      
 |      This is important for the RNN layer to invoke this in it call() method so
 |      that the cached mask is cleared before calling the cell.call(). The mask
 |      should be cached across the timestep within the same batch, but shouldn't
 |      be cached between batches. Otherwise it will introduce unreasonable bias
 |      against certain index of data within the batch.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from tensorflow.python.keras.layers.recurrent.DropoutRNNCellMixin:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from tensorflow.python.keras.layers.recurrent.LSTM:
 |  
 |  get_config(self)
 |      Returns the config of the layer.
 |      
 |      A layer config is a Python dictionary (serializable)
 |      containing the configuration of a layer.
 |      The same layer can be reinstantiated later
 |      (without its trained weights) from this configuration.
 |      
 |      The config of a layer does not include connectivity
 |      information, nor the layer class name. These are handled
 |      by `Network` (one layer of abstraction above).
 |      
 |      Returns:
 |          Python dictionary.
 |  
 |  ----------------------------------------------------------------------
 |  Class methods inherited from tensorflow.python.keras.layers.recurrent.LSTM:
 |  
 |  from_config(config) from builtins.type
 |      Creates a layer from its config.
 |      
 |      This method is the reverse of `get_config`,
 |      capable of instantiating the same layer from the config
 |      dictionary. It does not handle layer connectivity
 |      (handled by Network), nor weights (handled by `set_weights`).
 |      
 |      Arguments:
 |          config: A Python dictionary, typically the
 |              output of get_config.
 |      
 |      Returns:
 |          A layer instance.
 |  
 |  ----------------------------------------------------------------------
 |  Readonly properties inherited from tensorflow.python.keras.layers.recurrent.LSTM:
 |  
 |  activation
 |  
 |  bias_constraint
 |  
 |  bias_initializer
 |  
 |  bias_regularizer
 |  
 |  dropout
 |  
 |  implementation
 |  
 |  kernel_constraint
 |  
 |  kernel_initializer
 |  
 |  kernel_regularizer
 |  
 |  recurrent_activation
 |  
 |  recurrent_constraint
 |  
 |  recurrent_dropout
 |  
 |  recurrent_initializer
 |  
 |  recurrent_regularizer
 |  
 |  unit_forget_bias
 |  
 |  units
 |  
 |  use_bias
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from tensorflow.python.keras.layers.recurrent.RNN:
 |  
 |  __call__(self, inputs, initial_state=None, constants=None, **kwargs)
 |      Wraps `call`, applying pre- and post-processing steps.
 |      
 |      Arguments:
 |        *args: Positional arguments to be passed to `self.call`.
 |        **kwargs: Keyword arguments to be passed to `self.call`.
 |      
 |      Returns:
 |        Output tensor(s).
 |      
 |      Note:
 |        - The following optional keyword arguments are reserved for specific uses:
 |          * `training`: Boolean scalar tensor of Python boolean indicating
 |            whether the `call` is meant for training or inference.
 |          * `mask`: Boolean input mask.
 |        - If the layer's `call` method takes a `mask` argument (as some Keras
 |          layers do), its default value will be set to the mask generated
 |          for `inputs` by the previous layer (if `input` did come from
 |          a layer that generated a corresponding mask, i.e. if it came from
 |          a Keras layer with masking support.
 |      
 |      Raises:
 |        ValueError: if the layer's `call` method returns None (an invalid value).
 |        RuntimeError: if `super().__init__()` was not called in the constructor.
 |  
 |  build(self, input_shape)
 |      Creates the variables of the layer (optional, for subclass implementers).
 |      
 |      This is a method that implementers of subclasses of `Layer` or `Model`
 |      can override if they need a state-creation step in-between
 |      layer instantiation and layer call.
 |      
 |      This is typically used to create the weights of `Layer` subclasses.
 |      
 |      Arguments:
 |        input_shape: Instance of `TensorShape`, or list of instances of
 |          `TensorShape` if the layer expects a list of inputs
 |          (one instance per input).
 |  
 |  compute_mask(self, inputs, mask)
 |      Computes an output mask tensor.
 |      
 |      Arguments:
 |          inputs: Tensor or list of tensors.
 |          mask: Tensor or list of tensors.
 |      
 |      Returns:
 |          None or a tensor (or list of tensors,
 |              one per output tensor of the layer).
 |  
 |  compute_output_shape(self, input_shape)
 |      Computes the output shape of the layer.
 |      
 |      If the layer has not been built, this method will call `build` on the
 |      layer. This assumes that the layer will later be used with inputs that
 |      match the input shape provided here.
 |      
 |      Arguments:
 |          input_shape: Shape tuple (tuple of integers)
 |              or list of shape tuples (one per output tensor of the layer).
 |              Shape tuples can include None for free dimensions,
 |              instead of an integer.
 |      
 |      Returns:
 |          An input shape tuple.
 |  
 |  get_initial_state(self, inputs)
 |  
 |  reset_states(self, states=None)
 |      Reset the recorded states for the stateful RNN layer.
 |      
 |      Can only be used when RNN layer is constructed with `stateful` = `True`.
 |      Args:
 |        states: Numpy arrays that contains the value for the initial state, which
 |          will be feed to cell at the first time step. When the value is None,
 |          zero filled numpy array will be created based on the cell state size.
 |      
 |      Raises:
 |        AttributeError: When the RNN layer is not stateful.
 |        ValueError: When the batch size of the RNN layer is unknown.
 |        ValueError: When the input numpy array is not compatible with the RNN
 |          layer state, either size wise or dtype wise.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from tensorflow.python.keras.layers.recurrent.RNN:
 |  
 |  states
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:
 |  
 |  __delattr__(self, name)
 |      Implement delattr(self, name).
 |  
 |  __setattr__(self, name, value)
 |      Support self.foo = trackable syntax.
 |  
 |  add_loss(self, losses, **kwargs)
 |      Add loss tensor(s), potentially dependent on layer inputs.
 |      
 |      Some losses (for instance, activity regularization losses) may be dependent
 |      on the inputs passed when calling a layer. Hence, when reusing the same
 |      layer on different inputs `a` and `b`, some entries in `layer.losses` may
 |      be dependent on `a` and some on `b`. This method automatically keeps track
 |      of dependencies.
 |      
 |      This method can be used inside a subclassed layer or model's `call`
 |      function, in which case `losses` should be a Tensor or list of Tensors.
 |      
 |      Example:
 |      
 |      ```python
 |      class MyLayer(tf.keras.layers.Layer):
 |        def call(self, inputs):
 |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))
 |          return inputs
 |      ```
 |      
 |      This method can also be called directly on a Functional Model during
 |      construction. In this case, any loss Tensors passed to this Model must
 |      be symbolic and be able to be traced back to the model's `Input`s. These
 |      losses become part of the model's topology and are tracked in `get_config`.
 |      
 |      Example:
 |      
 |      ```python
 |      inputs = tf.keras.Input(shape=(10,))
 |      x = tf.keras.layers.Dense(10)(inputs)
 |      outputs = tf.keras.layers.Dense(1)(x)
 |      model = tf.keras.Model(inputs, outputs)
 |      # Activity regularization.
 |      model.add_loss(tf.abs(tf.reduce_mean(x)))
 |      ```
 |      
 |      If this is not the case for your loss (if, for example, your loss references
 |      a `Variable` of one of the model's layers), you can wrap your loss in a
 |      zero-argument lambda. These losses are not tracked as part of the model's
 |      topology since they can't be serialized.
 |      
 |      Example:
 |      
 |      ```python
 |      inputs = tf.keras.Input(shape=(10,))
 |      d = tf.keras.layers.Dense(10)
 |      x = d(inputs)
 |      outputs = tf.keras.layers.Dense(1)(x)
 |      model = tf.keras.Model(inputs, outputs)
 |      # Weight regularization.
 |      model.add_loss(lambda: tf.reduce_mean(d.kernel))
 |      ```
 |      
 |      Arguments:
 |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses
 |          may also be zero-argument callables which create a loss tensor.
 |        **kwargs: Additional keyword arguments for backward compatibility.
 |          Accepted values:
 |            inputs - Deprecated, will be automatically inferred.
 |  
 |  add_metric(self, value, name=None, **kwargs)
 |      Adds metric tensor to the layer.
 |      
 |      This method can be used inside the `call()` method of a subclassed layer
 |      or model.
 |      
 |      ```python
 |      class MyMetricLayer(tf.keras.layers.Layer):
 |        def __init__(self):
 |          super(MyMetricLayer, self).__init__(name='my_metric_layer')
 |          self.mean = tf.keras.metrics.Mean(name='metric_1')
 |      
 |        def call(self, inputs):
 |          self.add_metric(self.mean(x))
 |          self.add_metric(tf.reduce_sum(x), name='metric_2')
 |          return inputs
 |      ```
 |      
 |      This method can also be called directly on a Functional Model during
 |      construction. In this case, any tensor passed to this Model must
 |      be symbolic and be able to be traced back to the model's `Input`s. These
 |      metrics become part of the model's topology and are tracked when you
 |      save the model via `save()`.
 |      
 |      ```python
 |      inputs = tf.keras.Input(shape=(10,))
 |      x = tf.keras.layers.Dense(10)(inputs)
 |      outputs = tf.keras.layers.Dense(1)(x)
 |      model = tf.keras.Model(inputs, outputs)
 |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')
 |      ```
 |      
 |      Note: Calling `add_metric()` with the result of a metric object on a
 |      Functional Model, as shown in the example below, is not supported. This is
 |      because we cannot trace the metric result tensor back to the model's inputs.
 |      
 |      ```python
 |      inputs = tf.keras.Input(shape=(10,))
 |      x = tf.keras.layers.Dense(10)(inputs)
 |      outputs = tf.keras.layers.Dense(1)(x)
 |      model = tf.keras.Model(inputs, outputs)
 |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')
 |      ```
 |      
 |      Args:
 |        value: Metric tensor.
 |        name: String metric name.
 |        **kwargs: Additional keyword arguments for backward compatibility.
 |          Accepted values:
 |          `aggregation` - When the `value` tensor provided is not the result of
 |          calling a `keras.Metric` instance, it will be aggregated by default
 |          using a `keras.Metric.Mean`.
 |  
 |  add_update(self, updates, inputs=None)
 |      Add update op(s), potentially dependent on layer inputs.
 |      
 |      Weight updates (for instance, the updates of the moving mean and variance
 |      in a BatchNormalization layer) may be dependent on the inputs passed
 |      when calling a layer. Hence, when reusing the same layer on
 |      different inputs `a` and `b`, some entries in `layer.updates` may be
 |      dependent on `a` and some on `b`. This method automatically keeps track
 |      of dependencies.
 |      
 |      This call is ignored when eager execution is enabled (in that case, variable
 |      updates are run on the fly and thus do not need to be tracked for later
 |      execution).
 |      
 |      Arguments:
 |        updates: Update op, or list/tuple of update ops, or zero-arg callable
 |          that returns an update op. A zero-arg callable should be passed in
 |          order to disable running the updates by setting `trainable=False`
 |          on this Layer, when executing in Eager mode.
 |        inputs: Deprecated, will be automatically inferred.
 |  
 |  add_variable(self, *args, **kwargs)
 |      Deprecated, do NOT use! Alias for `add_weight`.
 |  
 |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)
 |      Adds a new variable to the layer.
 |      
 |      Arguments:
 |        name: Variable name.
 |        shape: Variable shape. Defaults to scalar if unspecified.
 |        dtype: The type of the variable. Defaults to `self.dtype`.
 |        initializer: Initializer instance (callable).
 |        regularizer: Regularizer instance (callable).
 |        trainable: Boolean, whether the variable should be part of the layer's
 |          "trainable_variables" (e.g. variables, biases)
 |          or "non_trainable_variables" (e.g. BatchNorm mean and variance).
 |          Note that `trainable` cannot be `True` if `synchronization`
 |          is set to `ON_READ`.
 |        constraint: Constraint instance (callable).
 |        use_resource: Whether to use `ResourceVariable`.
 |        synchronization: Indicates when a distributed a variable will be
 |          aggregated. Accepted values are constants defined in the class
 |          `tf.VariableSynchronization`. By default the synchronization is set to
 |          `AUTO` and the current `DistributionStrategy` chooses
 |          when to synchronize. If `synchronization` is set to `ON_READ`,
 |          `trainable` must not be set to `True`.
 |        aggregation: Indicates how a distributed variable will be aggregated.
 |          Accepted values are constants defined in the class
 |          `tf.VariableAggregation`.
 |        **kwargs: Additional keyword arguments. Accepted values are `getter`,
 |          `collections`, `experimental_autocast` and `caching_device`.
 |      
 |      Returns:
 |        The variable created.
 |      
 |      Raises:
 |        ValueError: When giving unsupported dtype and no initializer or when
 |          trainable has been set to True with synchronization set as `ON_READ`.
 |  
 |  apply(self, inputs, *args, **kwargs)
 |      Deprecated, do NOT use!
 |      
 |      This is an alias of `self.__call__`.
 |      
 |      Arguments:
 |        inputs: Input tensor(s).
 |        *args: additional positional arguments to be passed to `self.call`.
 |        **kwargs: additional keyword arguments to be passed to `self.call`.
 |      
 |      Returns:
 |        Output tensor(s).
 |  
 |  compute_output_signature(self, input_signature)
 |      Compute the output tensor signature of the layer based on the inputs.
 |      
 |      Unlike a TensorShape object, a TensorSpec object contains both shape
 |      and dtype information for a tensor. This method allows layers to provide
 |      output dtype information if it is different from the input dtype.
 |      For any layer that doesn't implement this function,
 |      the framework will fall back to use `compute_output_shape`, and will
 |      assume that the output dtype matches the input dtype.
 |      
 |      Args:
 |        input_signature: Single TensorSpec or nested structure of TensorSpec
 |          objects, describing a candidate input for the layer.
 |      
 |      Returns:
 |        Single TensorSpec or nested structure of TensorSpec objects, describing
 |          how the layer would transform the provided input.
 |      
 |      Raises:
 |        TypeError: If input_signature contains a non-TensorSpec object.
 |  
 |  count_params(self)
 |      Count the total number of scalars composing the weights.
 |      
 |      Returns:
 |          An integer count.
 |      
 |      Raises:
 |          ValueError: if the layer isn't yet built
 |            (in which case its weights aren't yet defined).
 |  
 |  get_input_at(self, node_index)
 |      Retrieves the input tensor(s) of a layer at a given node.
 |      
 |      Arguments:
 |          node_index: Integer, index of the node
 |              from which to retrieve the attribute.
 |              E.g. `node_index=0` will correspond to the
 |              first time the layer was called.
 |      
 |      Returns:
 |          A tensor (or list of tensors if the layer has multiple inputs).
 |      
 |      Raises:
 |        RuntimeError: If called in Eager mode.
 |  
 |  get_input_mask_at(self, node_index)
 |      Retrieves the input mask tensor(s) of a layer at a given node.
 |      
 |      Arguments:
 |          node_index: Integer, index of the node
 |              from which to retrieve the attribute.
 |              E.g. `node_index=0` will correspond to the
 |              first time the layer was called.
 |      
 |      Returns:
 |          A mask tensor
 |          (or list of tensors if the layer has multiple inputs).
 |  
 |  get_input_shape_at(self, node_index)
 |      Retrieves the input shape(s) of a layer at a given node.
 |      
 |      Arguments:
 |          node_index: Integer, index of the node
 |              from which to retrieve the attribute.
 |              E.g. `node_index=0` will correspond to the
 |              first time the layer was called.
 |      
 |      Returns:
 |          A shape tuple
 |          (or list of shape tuples if the layer has multiple inputs).
 |      
 |      Raises:
 |        RuntimeError: If called in Eager mode.
 |  
 |  get_losses_for(self, inputs)
 |      Deprecated, do NOT use!
 |      
 |      Retrieves losses relevant to a specific set of inputs.
 |      
 |      Arguments:
 |        inputs: Input tensor or list/tuple of input tensors.
 |      
 |      Returns:
 |        List of loss tensors of the layer that depend on `inputs`.
 |  
 |  get_output_at(self, node_index)
 |      Retrieves the output tensor(s) of a layer at a given node.
 |      
 |      Arguments:
 |          node_index: Integer, index of the node
 |              from which to retrieve the attribute.
 |              E.g. `node_index=0` will correspond to the
 |              first time the layer was called.
 |      
 |      Returns:
 |          A tensor (or list of tensors if the layer has multiple outputs).
 |      
 |      Raises:
 |        RuntimeError: If called in Eager mode.
 |  
 |  get_output_mask_at(self, node_index)
 |      Retrieves the output mask tensor(s) of a layer at a given node.
 |      
 |      Arguments:
 |          node_index: Integer, index of the node
 |              from which to retrieve the attribute.
 |              E.g. `node_index=0` will correspond to the
 |              first time the layer was called.
 |      
 |      Returns:
 |          A mask tensor
 |          (or list of tensors if the layer has multiple outputs).
 |  
 |  get_output_shape_at(self, node_index)
 |      Retrieves the output shape(s) of a layer at a given node.
 |      
 |      Arguments:
 |          node_index: Integer, index of the node
 |              from which to retrieve the attribute.
 |              E.g. `node_index=0` will correspond to the
 |              first time the layer was called.
 |      
 |      Returns:
 |          A shape tuple
 |          (or list of shape tuples if the layer has multiple outputs).
 |      
 |      Raises:
 |        RuntimeError: If called in Eager mode.
 |  
 |  get_updates_for(self, inputs)
 |      Deprecated, do NOT use!
 |      
 |      Retrieves updates relevant to a specific set of inputs.
 |      
 |      Arguments:
 |        inputs: Input tensor or list/tuple of input tensors.
 |      
 |      Returns:
 |        List of update ops of the layer that depend on `inputs`.
 |  
 |  get_weights(self)
 |      Returns the current weights of the layer.
 |      
 |      The weights of a layer represent the state of the layer. This function
 |      returns both trainable and non-trainable weight values associated with this
 |      layer as a list of Numpy arrays, which can in turn be used to load state
 |      into similarly parameterized layers.
 |      
 |      For example, a Dense layer returns a list of two values-- per-output
 |      weights and the bias value. These can be used to set the weights of another
 |      Dense layer:
 |      
 |      >>> a = tf.keras.layers.Dense(1,
 |      ...   kernel_initializer=tf.constant_initializer(1.))
 |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))
 |      >>> a.get_weights()
 |      [array([[1.],
 |             [1.],
 |             [1.]], dtype=float32), array([0.], dtype=float32)]
 |      >>> b = tf.keras.layers.Dense(1,
 |      ...   kernel_initializer=tf.constant_initializer(2.))
 |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))
 |      >>> b.get_weights()
 |      [array([[2.],
 |             [2.],
 |             [2.]], dtype=float32), array([0.], dtype=float32)]
 |      >>> b.set_weights(a.get_weights())
 |      >>> b.get_weights()
 |      [array([[1.],
 |             [1.],
 |             [1.]], dtype=float32), array([0.], dtype=float32)]
 |      
 |      Returns:
 |          Weights values as a list of numpy arrays.
 |  
 |  set_weights(self, weights)
 |      Sets the weights of the layer, from Numpy arrays.
 |      
 |      The weights of a layer represent the state of the layer. This function
 |      sets the weight values from numpy arrays. The weight values should be
 |      passed in the order they are created by the layer. Note that the layer's
 |      weights must be instantiated before calling this function by calling
 |      the layer.
 |      
 |      For example, a Dense layer returns a list of two values-- per-output
 |      weights and the bias value. These can be used to set the weights of another
 |      Dense layer:
 |      
 |      >>> a = tf.keras.layers.Dense(1,
 |      ...   kernel_initializer=tf.constant_initializer(1.))
 |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))
 |      >>> a.get_weights()
 |      [array([[1.],
 |             [1.],
 |             [1.]], dtype=float32), array([0.], dtype=float32)]
 |      >>> b = tf.keras.layers.Dense(1,
 |      ...   kernel_initializer=tf.constant_initializer(2.))
 |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))
 |      >>> b.get_weights()
 |      [array([[2.],
 |             [2.],
 |             [2.]], dtype=float32), array([0.], dtype=float32)]
 |      >>> b.set_weights(a.get_weights())
 |      >>> b.get_weights()
 |      [array([[1.],
 |             [1.],
 |             [1.]], dtype=float32), array([0.], dtype=float32)]
 |      
 |      Arguments:
 |          weights: a list of Numpy arrays. The number
 |              of arrays and their shape must match
 |              number of the dimensions of the weights
 |              of the layer (i.e. it should match the
 |              output of `get_weights`).
 |      
 |      Raises:
 |          ValueError: If the provided weights list does not match the
 |              layer's specifications.
 |  
 |  ----------------------------------------------------------------------
 |  Readonly properties inherited from tensorflow.python.keras.engine.base_layer.Layer:
 |  
 |  compute_dtype
 |      The dtype of the layer's computations.
 |      
 |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless
 |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of
 |      the weights.
 |      
 |      Layers automatically cast their inputs to the compute dtype, which causes
 |      computations and the output to be in the compute dtype as well. This is done
 |      by the base Layer class in `Layer.__call__`, so you do not have to insert
 |      these casts if implementing your own layer.
 |      
 |      Layers often perform certain internal computations in higher precision when
 |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output
 |      will still typically be float16 or bfloat16 in such cases.
 |      
 |      Returns:
 |        The layer's compute dtype.
 |  
 |  dtype
 |      The dtype of the layer weights.
 |      
 |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless
 |      mixed precision is used, this is the same as `Layer.compute_dtype`, the
 |      dtype of the layer's computations.
 |  
 |  dtype_policy
 |      The dtype policy associated with this layer.
 |      
 |      This is an instance of a `tf.keras.mixed_precision.Policy`.
 |  
 |  dynamic
 |      Whether the layer is dynamic (eager-only); set in the constructor.
 |  
 |  inbound_nodes
 |      Deprecated, do NOT use! Only for compatibility with external Keras.
 |  
 |  input
 |      Retrieves the input tensor(s) of a layer.
 |      
 |      Only applicable if the layer has exactly one input,
 |      i.e. if it is connected to one incoming layer.
 |      
 |      Returns:
 |          Input tensor or list of input tensors.
 |      
 |      Raises:
 |        RuntimeError: If called in Eager mode.
 |        AttributeError: If no inbound nodes are found.
 |  
 |  input_mask
 |      Retrieves the input mask tensor(s) of a layer.
 |      
 |      Only applicable if the layer has exactly one inbound node,
 |      i.e. if it is connected to one incoming layer.
 |      
 |      Returns:
 |          Input mask tensor (potentially None) or list of input
 |          mask tensors.
 |      
 |      Raises:
 |          AttributeError: if the layer is connected to
 |          more than one incoming layers.
 |  
 |  input_shape
 |      Retrieves the input shape(s) of a layer.
 |      
 |      Only applicable if the layer has exactly one input,
 |      i.e. if it is connected to one incoming layer, or if all inputs
 |      have the same shape.
 |      
 |      Returns:
 |          Input shape, as an integer shape tuple
 |          (or list of shape tuples, one tuple per input tensor).
 |      
 |      Raises:
 |          AttributeError: if the layer has no defined input_shape.
 |          RuntimeError: if called in Eager mode.
 |  
 |  losses
 |      List of losses added using the `add_loss()` API.
 |      
 |      Variable regularization tensors are created when this property is accessed,
 |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will
 |      propagate gradients back to the corresponding variables.
 |      
 |      Examples:
 |      
 |      >>> class MyLayer(tf.keras.layers.Layer):
 |      ...   def call(self, inputs):
 |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))
 |      ...     return inputs
 |      >>> l = MyLayer()
 |      >>> l(np.ones((10, 1)))
 |      >>> l.losses
 |      [1.0]
 |      
 |      >>> inputs = tf.keras.Input(shape=(10,))
 |      >>> x = tf.keras.layers.Dense(10)(inputs)
 |      >>> outputs = tf.keras.layers.Dense(1)(x)
 |      >>> model = tf.keras.Model(inputs, outputs)
 |      >>> # Activity regularization.
 |      >>> len(model.losses)
 |      0
 |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))
 |      >>> len(model.losses)
 |      1
 |      
 |      >>> inputs = tf.keras.Input(shape=(10,))
 |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')
 |      >>> x = d(inputs)
 |      >>> outputs = tf.keras.layers.Dense(1)(x)
 |      >>> model = tf.keras.Model(inputs, outputs)
 |      >>> # Weight regularization.
 |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))
 |      >>> model.losses
 |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]
 |      
 |      Returns:
 |        A list of tensors.
 |  
 |  metrics
 |      List of metrics added using the `add_metric()` API.
 |      
 |      Example:
 |      
 |      >>> input = tf.keras.layers.Input(shape=(3,))
 |      >>> d = tf.keras.layers.Dense(2)
 |      >>> output = d(input)
 |      >>> d.add_metric(tf.reduce_max(output), name='max')
 |      >>> d.add_metric(tf.reduce_min(output), name='min')
 |      >>> [m.name for m in d.metrics]
 |      ['max', 'min']
 |      
 |      Returns:
 |        A list of `Metric` objects.
 |  
 |  name
 |      Name of the layer (string), set in the constructor.
 |  
 |  non_trainable_variables
 |  
 |  non_trainable_weights
 |      List of all non-trainable weights tracked by this layer.
 |      
 |      Non-trainable weights are *not* updated during training. They are expected
 |      to be updated manually in `call()`.
 |      
 |      Note: This will not track the weights of nested `tf.Modules` that are not
 |      themselves Keras layers.
 |      
 |      Returns:
 |        A list of non-trainable variables.
 |  
 |  outbound_nodes
 |      Deprecated, do NOT use! Only for compatibility with external Keras.
 |  
 |  output
 |      Retrieves the output tensor(s) of a layer.
 |      
 |      Only applicable if the layer has exactly one output,
 |      i.e. if it is connected to one incoming layer.
 |      
 |      Returns:
 |        Output tensor or list of output tensors.
 |      
 |      Raises:
 |        AttributeError: if the layer is connected to more than one incoming
 |          layers.
 |        RuntimeError: if called in Eager mode.
 |  
 |  output_mask
 |      Retrieves the output mask tensor(s) of a layer.
 |      
 |      Only applicable if the layer has exactly one inbound node,
 |      i.e. if it is connected to one incoming layer.
 |      
 |      Returns:
 |          Output mask tensor (potentially None) or list of output
 |          mask tensors.
 |      
 |      Raises:
 |          AttributeError: if the layer is connected to
 |          more than one incoming layers.
 |  
 |  output_shape
 |      Retrieves the output shape(s) of a layer.
 |      
 |      Only applicable if the layer has one output,
 |      or if all outputs have the same shape.
 |      
 |      Returns:
 |          Output shape, as an integer shape tuple
 |          (or list of shape tuples, one tuple per output tensor).
 |      
 |      Raises:
 |          AttributeError: if the layer has no defined output shape.
 |          RuntimeError: if called in Eager mode.
 |  
 |  trainable_variables
 |      Sequence of trainable variables owned by this module and its submodules.
 |      
 |      Note: this method uses reflection to find variables on the current instance
 |      and submodules. For performance reasons you may wish to cache the result
 |      of calling this method if you don't expect the return value to change.
 |      
 |      Returns:
 |        A sequence of variables for the current module (sorted by attribute
 |        name) followed by variables from all submodules recursively (breadth
 |        first).
 |  
 |  trainable_weights
 |      List of all trainable weights tracked by this layer.
 |      
 |      Trainable weights are updated via gradient descent during training.
 |      
 |      Note: This will not track the weights of nested `tf.Modules` that are not
 |      themselves Keras layers.
 |      
 |      Returns:
 |        A list of trainable variables.
 |  
 |  updates
 |  
 |  variable_dtype
 |      Alias of `Layer.dtype`, the dtype of the weights.
 |  
 |  variables
 |      Returns the list of all layer variables/weights.
 |      
 |      Alias of `self.weights`.
 |      
 |      Note: This will not track the weights of nested `tf.Modules` that are not
 |      themselves Keras layers.
 |      
 |      Returns:
 |        A list of variables.
 |  
 |  weights
 |      Returns the list of all layer variables/weights.
 |      
 |      Note: This will not track the weights of nested `tf.Modules` that are not
 |      themselves Keras layers.
 |      
 |      Returns:
 |        A list of variables.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:
 |  
 |  activity_regularizer
 |      Optional regularizer function for the output of this layer.
 |  
 |  input_spec
 |      `InputSpec` instance(s) describing the input format for this layer.
 |      
 |      When you create a layer subclass, you can set `self.input_spec` to enable
 |      the layer to run input compatibility checks when it is called.
 |      Consider a `Conv2D` layer: it can only be called on a single input tensor
 |      of rank 4. As such, you can set, in `__init__()`:
 |      
 |      ```python
 |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)
 |      ```
 |      
 |      Now, if you try to call the layer on an input that isn't rank 4
 |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted
 |      error:
 |      
 |      ```
 |      ValueError: Input 0 of layer conv2d is incompatible with the layer:
 |      expected ndim=4, found ndim=1. Full shape received: [2]
 |      ```
 |      
 |      Input checks that can be specified via `input_spec` include:
 |      - Structure (e.g. a single input, a list of 2 inputs, etc)
 |      - Shape
 |      - Rank (ndim)
 |      - Dtype
 |      
 |      For more information, see `tf.keras.layers.InputSpec`.
 |      
 |      Returns:
 |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.
 |  
 |  stateful
 |  
 |  supports_masking
 |      Whether this layer supports computing a mask using `compute_mask`.
 |  
 |  trainable
 |  
 |  ----------------------------------------------------------------------
 |  Class methods inherited from tensorflow.python.module.module.Module:
 |  
 |  with_name_scope(method) from builtins.type
 |      Decorator to automatically enter the module name scope.
 |      
 |      >>> class MyModule(tf.Module):
 |      ...   @tf.Module.with_name_scope
 |      ...   def __call__(self, x):
 |      ...     if not hasattr(self, 'w'):
 |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))
 |      ...     return tf.matmul(x, self.w)
 |      
 |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose
 |      names included the module name:
 |      
 |      >>> mod = MyModule()
 |      >>> mod(tf.ones([1, 2]))
 |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>
 |      >>> mod.w
 |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,
 |      numpy=..., dtype=float32)>
 |      
 |      Args:
 |        method: The method to wrap.
 |      
 |      Returns:
 |        The original method wrapped such that it enters the module's name scope.
 |  
 |  ----------------------------------------------------------------------
 |  Readonly properties inherited from tensorflow.python.module.module.Module:
 |  
 |  name_scope
 |      Returns a `tf.name_scope` instance for this class.
 |  
 |  submodules
 |      Sequence of all sub-modules.
 |      
 |      Submodules are modules which are properties of this module, or found as
 |      properties of modules which are properties of this module (and so on).
 |      
 |      >>> a = tf.Module()
 |      >>> b = tf.Module()
 |      >>> c = tf.Module()
 |      >>> a.b = b
 |      >>> b.c = c
 |      >>> list(a.submodules) == [b, c]
 |      True
 |      >>> list(b.submodules) == [c]
 |      True
 |      >>> list(c.submodules) == []
 |      True
 |      
 |      Returns:
 |        A sequence of all submodules.
 |  
 |  ----------------------------------------------------------------------
 |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:
 |  
 |  __new__(cls, *args, **kwargs)
 |      Create and return a new object.  See help(type) for accurate signature.
```

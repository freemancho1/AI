# NLP - 영문 데이터 전처리

<br/><br/>

## 1. 데이터 다운로드

<br/>

### 1.1. kaggle 라이브러리 이용
```python
$ kaggle cometitions download -c word2vec-nlp-tutorial
```
> 대회명이 틀렸나 403에러 발생 <br/>
> 정상적으로 다운되면 4개의 파일이 생성된다.

<br/>

### 1.2. 직접 홈페이지에서 다운받기
> 홈페이지에 접속해 대회명을 입력해 다운받을 수 있다.

<br/>

## 2. 데이터 불러오기

<br/>

### 2.1. 압축 해제
```python
import zipfile

DATA_IN_PATH = '../DATA/word_popcorn/'
file_list = ['labeledTrainData.tsv.zip', 'unlabeledTrainData.tsv.zip', 'testData.tsv.zip']

for file in file_list:
    zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')
    zipRef.extractall(DATA_IN_PATH)
    zipRef.close()
```

<br/>

### 2.2. pandas로 읽기
> 파일을 판다스로 읽기 전에 먼저 간단히 파일이 어떻게 생겼는지 확인한다.
```python
$ head labeledTrainData.tsv
```
> header가 있는것을 알 수 있고, 당연히 tsv니 구분자는 tab임을 알 수 있으며, 값들이 큰 따옴표로 엮어 있음을 알 수 있다.

<br/>

```python
import os
import re
import csv
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import pandas_profiling

from nltk.corpus import stopwords
from bs4 import BeautifulSoup
from gensim.models import word2vec
from wordcloud import WordCloud
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

%matplotlib inline

train_df = pd.read_csv(DATA_IN_PATH+'labeledTrainData.tsv', header=0, delimiter='\t', quoting=csv.QUOTE_ALL)
# header=0: 첫 번째 줄은 header로 사용한다. 기본값은 infer(추론), header=None는 첫 번째 줄부터 바로 데이터로 활용
# delimiter='\t': 'sep='과 동일함
# quoting=2: 처음과 끝의 큰 따옴표는 무시
# 자세한 내용은 아래 참조
# print(pd.read_csv.__doc__)
```
<img src="https://user-images.githubusercontent.com/31339365/102861593-45541c00-4473-11eb-9be7-11cd8ffb28ec.png"></img>

<br/>

## 3. 데이터 분석

<br/>

### 3.1. pandas_profiling으로 분석
```python
import pandas_profiling

train_df_pp = train_df.profile_report()
train_df_pp.to_file('./train_df_pp.html')
```
##### 결과
<img src="https://user-images.githubusercontent.com/31339365/102862211-15594880-4474-11eb-89f8-fb6aa3014f18.png"></img>
> 각 컬럼의 정보, 처음과 끝 데이터 샘플 등 다양한 정보를 볼 수 있다.

<br/>

### 3.2. 기본적인 함수로 분석
> 여기서는 numpy를 이용해 pandas 데이터를 다루는 다양한 방법을 확인한다.

#### 앞/뒤 데이터 보기
```python
train_df.head()
train_df.tail()
```

#### 파일의 크기 확인
```python
import os

print('파일 크기:')
for file in os.listdir(DATA_IN_PATH):
    if 'tsv' in file:
        print(f'{file.ljust(30)} {str(round(os.path.getsize(DATA_IN_PATH+file)/(1024*1024), 2))} MBytes')

## 결과
# 파일 크기:
# labeledTrainData.tsv           32.0 MBytes
# testData.tsv                   31.21 MBytes
# unlabeledTrainData.tsv         64.16 MBytes
```

#### 학습 데이터 크기 및 각 댓글의 길이 확인
```python
print(f'전체 학습 데이터의 개수: {len(train_df)}')

# 각 댓글의 길이 저장
train_review_length = train_df['review'].apply(len)
train_review_length.head()

## 결과
# 전체 학습 데이터의 개수: 25000
# 0    2304
# 1     948
# 2    2451
# 3    2247
# 4    2233
# Name: review, dtype: int64
```

#### 리뷰 길이 분석
##### 이미지로 분석
```python
plt.figure(figsize=(12,5))
plt.hist(train_review_length, bins=200, alpha=0.3, color='r', label='word')
plt.yscale('log', nonpositive='clip')
plt.title('Log-Histogram of length of review')
plt.xlabel('Length of review')
plt.ylabel('Number of review')
```
<img src="https://user-images.githubusercontent.com/31339365/102867067-8d773c80-447b-11eb-9425-07b0ccec3336.png"></img>

##### 최대/평균/최대 길이 확인
```python
print(f'리뷰 길이 최소값: {np.min(train_review_length)}')
print(f'리뷰 길이 평균값: {np.mean(train_review_length)}')
print(f'리뷰 길이 최대값: {np.max(train_review_length)}')
print(f'리뷰 길이 중앙값: {np.median(train_review_length)}')

## 결과
# 리뷰 길이 최소값: 54
# 리뷰 길이 평균값: 1329.71056
# 리뷰 길이 최대값: 13710
# 리뷰 길이 중앙값: 983.0
```

#### 리뷰 단어 갯수 분석
```python
train_word_counts = train_df['review'].apply(lambda x: len(x.split(' ')))

print(f'리뷰 단어 갯수 최소값: {np.min(train_word_counts)}')
print(f'리뷰 단어 갯수 평균값: {np.mean(train_word_counts)}')
print(f'리뷰 단어 갯수 중앙값: {np.median(train_word_counts)}')
print(f'리뷰 단어 갯수 최대값: {np.max(train_word_counts)}')

## 결과
# 리뷰 단어 갯수 최소값: 10
# 리뷰 단어 갯수 평균값: 233.78624
# 리뷰 단어 갯수 중앙값: 174.0
# 리뷰 단어 갯수 최대값: 2470
```

#### WordCloud
```python
from wordcloud import WordCloud

train_df_cloud = WordCloud(width=1000, height=600).generate(' '.join(train_df['review']))
plt.figure(figsize=(20, 12))
plt.imshow(train_df_cloud)
plt.axis('off')
```
<img src="https://user-images.githubusercontent.com/31339365/102867817-a9c7a900-447c-11eb-9276-0a9a5456cf6e.png"></img>
> 이미지를 보면 "br"이 많이 있는것을 볼 수 있는데, 이는 HTML 태그로 좀 있음 제거된다.

#### 라벨 분포 확인
```python
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(6,3)
sns.countplot(x=train_df['sentiment'])
```
<img src="https://user-images.githubusercontent.com/31339365/102870728-c960d080-4480-11eb-8170-8bd908ac8e20.png"></img>
```python
print(f'긍정 리뷰 갯수: {train_df["sentiment"].value_counts()[1]}')
print(f'부정 리뷰 갯수: {train_df["sentiment"].value_counts()[0]}')

## 결과
# 긍정 리뷰 갯수: 12500
# 부정 리뷰 갯수: 12500
```

#### 물음표/마침표 여부, 첫글자 대문자 여부, 숫자가 있는지 여부 등 확인
> 다양한 형태의 pandas 데이터 처리 방법 확인
```python
qmark = np.mean(train_df['review'].apply(lambda x: x[-1]=='?'))
qmark2 = np.mean(train_df['review'].apply(lambda x: '?' in x))
period = np.mean(train_df['review'].apply(lambda x: x[-1]=='.'))
first_capital = np.mean(train_df['review'].apply(lambda x: x[0].isupper()))
capitals = np.mean(train_df['review'].apply(lambda x: max([y.isupper() for y in x])))
digits = np.mean(train_df['review'].apply(lambda x: max([y.isdigit() for y in x])))
digits_re = np.mean(train_df['review'].apply(lambda x: 1 if re.search('\d+', x) is not None else 0))

print(f'{"물음표로 끝나는 질문".ljust(30)} \t: {qmark*100}')
print(f'{"물음표가 있는 질문".ljust(30)} \t: {qmark2*100}')
print(f'{"마침표로 끝나는 질문".ljust(30)} \t: {period*100}')
print(f'{"첫 글자가 대문자인 질문".ljust(30)} \t: {first_capital*100}')
print(f'{"대문자가 있는 질문".ljust(30)} \t: {capitals*100}')
print(f'{"숫자가 있는 질문".ljust(30)} \t: {digits*100}')
print(f'{"숫자가 있는 질문2".ljust(30)} \t: {digits_re*100}')

## 결과
# 물음표로 끝나는 질문                     : 1.9
# 물음표가 있는 질문                       : 29.552
# 마침표로 끝나는 질문                     : 72.048
# 첫 글자가 대문자인 질문                   : 92.84400000000001
# 대문자가 있는 질문                       : 99.592
# 숫자가 있는 질문                        : 56.65599999999999
# 숫자가 있는 질문2                       : 56.65599999999999
```

<br/>

## 4. 데이터 전처리

<br/>

### 4.1. 주로 사용하는 라이브러리들
> * 데이터 가공: numpy, pandas, json
> * 데이터 정제: re, bs4(Beautiful Soup)
> * 불용어 제거: NLTK의 stopwords
> * ??? : tensorflow의 pad_sequences, Tokenizer

```python
import re
import pandas as pd
import numpy as np
import json
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
```

<br/>

### 4.2. 데이터 전처리 방향 결정을 위한 데이터 세부 확인(분석단계와 유사)
```python
print(train_df['review'][0])
```
> With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.&lt;br /&gt;&lt;br /&gt;Visually impressive bu... <br/>

> * HTML 태그, 특수문자 및 문장부호 등이 많이 보인다.  
> * HTML 태그는 &lt;h1&gt; 같이 중요한 의미가 있는 것들도 있지만 여기서 보이는 &lt;br/&gt;는 단순 개행을 위미하기 때문에 제거한다.
> * 특수문자 및 문장부호는 문장의 의미에 크게 영향을 주지 않기 때문에 제거한다.

<br/>

### 4.3. 불용 데이터 제거
> 불용 데이터 제거는 다음에 나오는 불용어 제거와는 의미가 다르다. 불용 데이터 제거는 데이터 중에 의미없는 데이터를 제거하는 것이고, 불용어 제거는 문장에서 자주 출현하나 전체적인 의미에 큰 영향이 없는 단어를 제거하는 것이다.
> * BeautifulSoup - HTML 제거
> * re - 특수문자 제거
```python
review = train_df['review'][0]                                # 리뷰 중 하나를 가져온다.

review_text = BeautifulSoup(review,'html.parser').get_text()  # HTML 태그 제거
review_text = re.sub('[^A-z]', ' ', review_text )             # 영어 문자를 제외한 나머지는 모두 공백으로 바꾼다.
print(review_text)                                            # 여기서는 일단 숫자는 의미가 없으니 같이 제거함

# review_text = re.sub('[^A-z0-9]', ' ', review_text )          # 숫자 포함
```

<br/>

### 4.4. 불용어 제거
> 불용어는 불용어 사전을 이용해 제거한다.  사용자가 직접 정의할 수 있지만, 고려할 사항이 너무 많기 때문에 어디선가 정의해 둔 불용어 사전을 이용한다.
> * 여기서는 NLTK의 불용어 사전을 사용한다.
> * 데이터에서 사전에 정의된 모든 단어를 삭제하면 된다.
> * NLTK 불용어 사전은 모두 소문자로 구성되어 있기 때문에 데이터를 소문자로 변환 후 제거한다.
> * NLTK 라이브러리를 사용하기 위해서는 환경파일이 있는 곳에 데이터가 있거나 링크를 만들어야 한다.
>   * $ ln -s ~/projects/tf24gp38/data/nltk_data/ ~/anaconda3/envs/tf24gp38/nltk_data
```python
stop_words = set(stopwords.words('english'))  # 영문 불용어 set 생성
review_text = review_text.lower()             # 소문자 변환
review_words = review_text.split()            # 단어별로 분류해 리스트 생성
review_words = \                              # 불용어 제거
    [w for w in review_words if not w in stop_words]
print(review_words)
```
##### 다시 하나의 문장으로 변환
```python
review_clean = ' '.join(review_words)
```

<br/>

### 4.5. 불용 데이터 제거(2.4.3)와 불용어 제거(2.4.4)를 전체 데이터 대상으로 수행
```python
def pp_remove_unnecessary_data(review, stopwords_dic=None):
    review_text = BeautifulSoup(review, "html.parser").get_text()
    review_text = re.sub('[^A-z]', ' ', review_text)
    
    review_words = review_text.lower().split()
    
    if stopwords_dic:
        review_words = [w for w in review_words if not w in stopwords_dic]
        
    review_clean = ' '.join(review_words)
    
    return review_clean
    
stopwords_dic = set(stopwords.words('english'))
clean_train_reviews = []

for review in train_df['review']:
    clean_train_reviews.append(pp_remove_unnecessary_data(review, stopwords_dic=stopwords_dic))
    
print(clean_train_reviews[0])
```

<br/>

### 2.4.6. 단어 임베딩 작업
> 인공지능 모델이 단어를 이해할 수 있도록 리뷰 각 단어들을 벡터화 시키는 작업을 수행한다.
> * 단, 모델에 따라서는 벡터화 된 단어보다 단어 자체를 사용하는 경우도 있으니 아래와 같이 정제된 단어를 별도로 저장해 둔다.
```python
clean_train_df = pd.DataFrame({'review': clean_train_reviews, 'sentiment': train_df['sentiment']})
```
> index는 별도로 사용하지 않기 때문에 저장하지 않았다.

#### 단어 임베딩 단계에서 수행하는 일
> * 각 단어에 인덱스를 부여한다.
> * 모델 입력값의 차원은 대부분 동일하기 때문에 단어의 숫자를 동일하게 하는 padding 작업을 수행한다.
##### 인덱스 부여
```python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(clean_train_reviews)                            #1
review_sequences = tokenizer.texts_to_sequences(clean_train_reviews)
print(review_sequences[0])
```
> [404, 70, 419, 8845, 507, 2461, 115, 54, 874, 517, 178, 18759, 178, 11281, 165, 78, 14, 663, 2462, 117, 92, 10, 500, 4082, 165, 22, 210, 582, 2337, 1195, 11281, 71, 4835, 71, 636, 2, 253, 70, 11, 302, 1665, 487, 1145, 3271, 8845, 411, 794, 3348, 17, 441, 601, 1501, 15, 4433, 1854, 999, 146, 342, 1443, 744, 2429, 4, 8845, 418, 70, 638, 69, 237, 94, 542, 8845, 26150, 26151, 120, 1, 8845, 323, 8, 47, 20, 323, 167, 10, 207, 634, 636, 2, 116, 291, 382, 121, 15592, 3321, 1502, 575, 735, 10050, 924, 11619, 823, 1240, 1409, 360, 8845, 221, 15, 577, 8845, 22306, 2278, 13476, 735, 10050, 27, 28709, 340, 16, 41, 18760, 1501, 388, 11282, 165, 3970, 8845, 115, 628, 500, 79, 4, 8845, 1431, 380, 2167, 114, 1923, 2508, 575, 17, 60, 100, 4884, 5112, 260, 1269, 26152, 15, 575, 494, 745, 638, 632, 3, 394, 164, 446, 114, 616, 3272, 1161, 685, 48, 1176, 224, 1, 16, 4, 8845, 3, 508, 62, 25, 16, 641, 133, 231, 95, 7448, 601, 3445, 8845, 37370, 1867, 1, 128, 342, 1443, 247, 3, 866, 16, 42, 1488, 998, 2337, 12, 550, 386, 718, 6938, 12, 41, 16, 158, 362, 4401, 3394, 41, 87, 225, 438, 207, 254, 117, 3, 18761, 18762, 316, 1357]

<br/>

> * #1: 토큰을 만들기 위한 인덱스는 훈련/시험용 모든 데이터를 기반으로 만들어야 된다.
> * 결과를 보면 텍스트로 되어 있던 첫번째 리뷰가 단어 인덱스로 구성된 벡터로 되어 있음을 알 수 있다.

##### 단어 사전 확인
> * 모든 리뷰의 단어들에 인덱스가 붙었는데, 이 인덱스가 어떤 단어인지 확인하기 위해서는 단어사전이 필요하다.
```python
word_vocab = tokenizer.word_index
word_vocab['<PAD>'] = 0                 #1
print(len(word_vocab))

## 결과
# 74203
```
> * #1: word_index에는 없는 값에 대한 정의가 필요하다. 이를 "&lt;PAD&gt;"로 정의하고 값을 "0"으로 입력하면 단어사전이 완성된다.
> * 단어사전과 단어사전의 갯수도 나중에 사용하기 때문에 별도로 저장한다.
```python
data_configs = {'vocab': word_vocab, 'vocab_size': len(word_vocab)+1}
```

<br/>

### 4.7. 입력 차원 동일하게 수정
> 리뷰의 단어 갯수에 따라 벡터의 차원수가 결정되는 데이터 셋이기 때문에 입력 차원수가 다 다르다. 동일하게 만들어 줘야 한다. <br/>
> 입력 차원수의 결정은 가장 큰 벡터의 차원수를 가지고 하던지 차원 갯수가 중앙인 중앙값을 가지고 할 수 있다. <br/>
> 처리 속도 및 정확도에 영향을 주기 때문에 적절히 상황에 따라 사용하면 된다. 여기서는 **정리된 단어 갯수의 중앙값**을 이용한다.
```python
clean_word_counts = clean_train_df['review'].apply(lambda x: len(x.split(' ')))
MAX_SEQUENCE_LENGTH = int(np.median(clean_word_counts))    # 정리된 단어 개수의 중앙값 사용, 전체 단어 개수의 중앙값은 174
train_inputs = pad_sequences(review_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')  #1
print(f'Shape of train data: {train_inputs.shape}')
print(f'Type of train data: {type(train_inputs)}')

## 결과
# Shape of train data: (25000, 89)
# Type of train data: <class 'numpy.ndarray'>
```
> * #1: review_sequences에 들어있는 차원을 maxlen만큼으로 동일하게 처리하는 명령
>   * padding은 차원이 maxlen보다 적으면 무효의 값을 어떻게 넣을지 결정한다.
```python
tf.keras.preprocessing.sequence.pad_sequences(  # 여기 표현된 값들은 default값이다.
    sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)
```

<br/>

### 4.8. 라벨값을 넘파이 형태로 변환
> 위 결과에서 최종적으로 만들어진 trina_inputs의 데이터 타입이 넘파이 형태이기 때문에, 라벨도 넘파이 형태로 변환해서 나중에 모델에 입력한다.
```python
train_labels = np.array(train_df['sentiment'])
print(f'Shape of train labels: {train_labels.shape}')
print(f'Type of train labels: {type(train_labels)}')

## 결과
# Shape of train labels: (25000,)
# Type of train labels: <class 'numpy.ndarray'>
```

<br/>

### 4.9. 전처리 데이터 저장
> 전처리가 끝나면 모델에서 사용하기 위해 아래와 같은 데이터를 저장한다.
> * 정제된 텍스트 데이터
> * 벡터화된 텍스트 데이터
> * 정답 라벨
> * 데이터 정보(단어사전, 전체 단우 개수)
> 일반적으로, 텍스트 데이터는 csv로, 벡터데이터와 라벨은 numpy파일로, 데이터정보는 디셔너리형태이기 때문에 json형태로 저장한다.
```python
MODEL_DATA = DATA_IN_PATH + 'MODELING/'
if not os.path.exists(MODEL_DATA):
    os.makedirs(MODEL_DATA)
    
TRAIN_INPUT_DATA = 'train_input.npy'
TRAIN_LABEL_DATA = 'train_label.npy'
TRAIN_CLEAN_DATA = 'train_clean.csv'
TRAIN_CONFIG_DATA = 'train_config.json'

np.save(open(MODEL_DATA+TRAIN_INPUT_DATA, 'wb'), train_inputs)
np.save(open(MODEL_DATA+TRAIN_LABEL_DATA, 'wb'), train_labels)

clean_train_df.to_csv(MODEL_DATA+TRAIN_CLEAN_DATA, index=False)

json.dump(data_configs, open(MODEL_DATA+TRAIN_CONFIG_DATA, 'w'), ensure_ascii=False)
```

<br/>

### 4.10. 평가 데이터 전처리
> 평가 데이터도 위와 동일한 처리를 수행해야 나중에 학습된 인공지능 모델이 정확하게 평가를 할 수 있다. <br/>
> 평가 데이터를 만들때는 아래 사항을 조심해야 한다.
> * 여기에 있는 데이터의 평가 데이터는 라벨이 없기 때문에 저장할 필요가 없다. (평가 데이터의 라벨이 있는 경우에는 저장해서 확인하면 좋다)
> * 답안을 제출해야 하기 때문에 훈련 데이터에서 저장하지 않았던 id값을 저장해야 한다.
> * **가장 중요** 평가 데이터의 벡터화를 위해 토크나이저를 실행할 경우, 반드시 훈련 데이터의 토크나이저를 사용해야 한다. 그렇지 않으면 모델이 평가를 제대로 하지 못하며, 이때 사전에 없는 데이터는 자동으로 &lt;PAD&gt; 데이터가 들어간다.
```python
test_df = pd.read_csv(DATA_IN_PATH+'testData.tsv', header=0, delimiter='\t', quoting=csv.QUOTE_ALL)

clean_test_reviews = []
for review in test_df['review']:
    clean_test_reviews.append(pp_remove_unnecessary_data(review, stopwords_dic=stopwords_dic))
clean_test_df = pd.DataFrame({'review': clean_test_reviews, 'id': test_df['id']})
test_ids = np.array(test_df['id'])

test_sequences = tokenizer.texts_to_sequences(clean_test_reviews)
test_inputs = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

TEST_INPUT_DATA = 'test_input.npy'
TEST_ID_DATA = 'test_id.npy'
TEST_CLEAN_DATA = 'test_clean.csv'

np.save(open(MODEL_DATA+TEST_INPUT_DATA, 'wb'), test_inputs)
np.save(open(MODEL_DATA+TEST_ID_DATA, 'wb'), test_ids)

clean_test_df.to_csv(MODEL_DATA+TEST_CLEAN_DATA, index=False)
```

<br/><br/>

## 5. 데이터 전처리2
> 4장에서는 tensorflow을 이용한 데이터 전처리를 확인해 봤고, 여기서는 scikit-learn및 word2vec을 이용한 데이터 전처리를 수행한다. <br/>
> scikit-learn에서 데이터를 전처리하는 방법은 다양하지만, 여기서는 TF-IDF와 Count을 이용한 방법에 대해 알아본다. <br/>
> 여기서부터는 변수명이 약간 다른데, 직관적이니 이해하는데는 문제가 없으리라 생각된다.

<br/>

### 5.1. TF-IDF를 이용한 데이터 벡터화
```python
sk_train_reviews, sk_train_sentiment = list(clean_train_df['review']), list(original_train_df['sentiment'])

sk_train_vectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True, ngram_range=(1,3), max_features=5000)
sk_train_vec_reviews = sk_train_vectorizer.fit_transform(sk_train_reviews)

print(sk_train_vec_reviews.get_shape)
print(sk_train_vec_reviews)

## 결과
# <bound method spmatrix.get_shape of <25000x5000 sparse matrix of type '<class 'numpy.float64'>'
#	with 23257034 stored elements in Compressed Sparse Row format>>
#
#  (0, 1828)	0.01638857003265972
#  (0, 2764)	0.017594053361187124
#  : :
#  (0, 4478)	0.029318830744143553
#  (0, 4315)	0.018765698611523496
#  (0, 3005)	0.02557151093687765
#  (0, 4449)	0.018635906631810674   #1: 1*max_features(5000)
#  :	:
#  (24999, 2929)	0.04935323015428267
#  (24999, 1579)	0.06460379624027954
```
> 결과를 보면 25000 x max_features로 되어있는것을 볼 수 있다. 이 파일을 나누고 저장하자
> * 텐서플로 토크나이저는 단어에 인덱스를 주고 그 인덱스를 기준으로 벡터를 만들었다면,
> * TF-IDF 방법은 값에 가중치를 준다.
```python
RANDOM_SEED = 42
TEST_SPLIT = 0.3

sk_train_labels = np.array(sk_train_sentiment)

sk_train_vec, sk_vaild_vec, sk_train_label, sk_vaild_label = \
    train_test_split(sk_train_vec_reviews, sk_train_labels, test_size=TEST_SPLIT, random_state=RANDOM_SEED)
    
sk_test_reviews = list(clean_test_df['review'])
sk_test_vec_reviews = sk_train_vectorizer.fit_transform(sk_test_reviews)

np.save(open(CLEAN_DATA_PATH+'train_datas_tfidf.npy', 'wb'), sk_train_vec)
np.save(open(CLEAN_DATA_PATH+'train_labels_tfidf.npy', 'wb'), sk_train_label)
np.save(open(CLEAN_DATA_PATH+'vaild_datas_tfidf.npy', 'wb'), sk_vaild_vec)
np.save(open(CLEAN_DATA_PATH+'vaild_labels_tfidf.npy', 'wb'), sk_vaild_label)
np.save(open(CLEAN_DATA_PATH+'test_datas_tfidf.npy', 'wb'), sk_test_vec_reviews)
```

<br/>

### 5.2. Counter를 이용한 데이터 벡터화
```python
cv_train_vectorizer = CountVectorizer(analyzer='word', max_features=5000)
cv_train_vec_reviews = cv_train_vectorizer.fit_transform(sk_train_reviews)
print(cv_train_vec_reviews)

## 결과
#  (0, 4267)	1
#  (0, 1905)	3
#  (0, 2874)	1
#  (0, 4181)	1
#  (0, 2590)	1
#   :	:
#  (24999, 3782)	1
#  (24999, 3521)	1
```
> 결과값이 TF-IDF와 유사한데, 여기의 값은 단어가 문장에서 반복해 나오는 횟수이고, TF-IDF는 약간의 공식이 들어간 수치이다.
```python
RANDOM_SEED = 42
TEST_SPLIT = 0.3

sk_train_labels = np.array(sk_train_sentiment)

cv_train_vec, cv_vaild_vec, cv_train_label, cv_vaild_label = \
    train_test_split(cv_train_vec_reviews, sk_train_labels, test_size=TEST_SPLIT, random_state=RANDOM_SEED)

cv_test_vec_reviews = cv_train_vectorizer.fit_transform(sk_test_reviews)

np.save(open(CLEAN_DATA_PATH+'train_datas_cv.npy', 'wb'), cv_train_vec)
np.save(open(CLEAN_DATA_PATH+'train_labels_cv.npy', 'wb'), cv_train_label)
np.save(open(CLEAN_DATA_PATH+'vaild_datas_cv.npy', 'wb'), cv_vaild_vec)
np.save(open(CLEAN_DATA_PATH+'vaild_labels_cv.npy', 'wb'), cv_vaild_label)
np.save(open(CLEAN_DATA_PATH+'test_datas_cv.npy', 'wb'), cv_test_vec_reviews)
```

<br/>

### 5.3. word2vec을 활용한 데이터 벡터화
#### word2vec 학습
```python
import numpy as np
import pandas as pd
from gensim.models import word2vec
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from sklearn.decomposition import PCA

DATA_PATH = './data/word_popcorn/'
ORIGINAL_DATA_PATH = DATA_PATH + 'original/'
CLEAN_DATA_PATH = DATA_PATH + 'clean/'

train_data = pd.read_csv(CLEAN_DATA_PATH+'train_datas_clean.csv')
reviews = list(train_data['review'])
sentiment = list(train_data['sentiment'])

total_review_words = []
for review in reviews:
    total_review_words.append(review.split())
```
> word2vec은 말 그대로 전체 단어를 인덱싱해서 처리해야 하기 때문에 텐서플로의 토크나이징과 유사하다. <br/>
> word2vec은 일반함수가 아니고 또 하나의 인공지능 모델이기 때문에 하이퍼파라미터를 설정해야 한다.
```python
num_features = 300     # 워드벡터의 특징값 수로, 각 단어에 대한 임베딩된 벡터의 차원 수
min_word_count = 40    # 단어에 대한 최소 빈도 수, 의미있는 빈도수의 단어만 선택하기 위해 적은 빈도 수의 단어는 학습에서 제외
num_workers = 4        # 모델 학습에 사용할 프로세스 개수
context = 10           # 컨텍스트 윈도 크기
downsampling = 1e-3    # 다운 샘플링 비율(빠른 처리를 위해 사용, 보통 0.001 사용)

model_word2vec = word2vec.Word2Vec(
    total_review_words, workers=num_workers, size=num_features, 
    min_count=min_word_count, window=context, sample=downsampling)
```

#### 모델 저장
```python
model_name = 'w2v_300features_40minwords_10context'
model_word2vec.save(model_name)
```
> 모델을 저장할 경우에는 모델명에 학습정보를 넣어 쉽게 알아볼 수 있도록 수정

#### 시각화
```python
w2v_word_vectors = model_word2vec.wv
w2v_word_vocabs = w2v_word_vectors.vocab.keys()
w2v_word_vectors_list = [w2v_word_vectors[v] for v in w2v_word_vocabs]

fm._rebuild()
plt.rc('font', family='NanumGothic')

def plot_2d_graph(vocabs, xs, ys):
    plt.figure(figsize=(15,10))
    plt.scatter(xs, ys, marker='o')
    for i, v in enumerate(vocabs):
        plt.annotate(v, xy=(xs[i], ys[i]))

pca = PCA(n_components=2)
xys = pca.fit_transform(w2v_word_vectors_list)
xs = xys[:,0]
ys = xys[:,1]
plot_2d_graph(w2v_word_vocabs, xs, ys)
``` 
> 이 부분은 나중에 더 좋은 시각화 방법을 찾아야 할 듯

#### 동일 차원으로 입력값 조정
```python
def get_features(review, model, index2word_set, num_features):
    feature_vector = np.zeros((num_features), dtype=np.float32)        # 출력값 초기화
    
    num_words = 0
    for w in review:
        if w in index2word_set:
            num_words += 1
            feature_vector = np.add(feature_vector, model[w])
    
    feature_vector = np.divide(feature_vector, num_words)
    return feature_vector

def get_dataset(reviews, model, num_features):
    dataset = []
    index2word_set = set(model.wv.index2word)
    
    for review in reviews:
        dataset.append(get_features(review, model, index2word_set, num_features))
        
    reviewFeatureVecs = np.stack(dataset)
    return reviewFeatureVecs

w2v_train_data = get_dataset(total_review_words, model_word2vec, num_features)
print(w2v_train_data)

## 결과 (25,000 * 300=num_features)
# [[-0.07807604 -0.3984975   0.04497184 ...  0.02890903  0.10936121
#    0.02662118]
#  [-0.18757778 -0.3733353   0.36018553 ... -0.08399471  0.00996554
#   -0.01976611]
#  [ 0.10790479  0.00130515  0.13631764 ... -0.07322755 -0.0032762
#    0.15794976]
#  ...
```

#### 시험 데이터 전처리 및 저장
```python
test_data = pd.read_csv(CLEAN_DATA_PATH+'test_datas_clean.csv')
reviews = list(test_data['review'])

total_review_words = []
for review in reviews:
    total_review_words.append(review.split())
    
w2v_test_data = get_dataset(total_review_words, model_word2vec, num_features)

np.save(open(CLEAN_DATA_PATH+'train_datas_w2v.npy', 'wb'), w2v_train_data)
np.save(open(CLEAN_DATA_PATH+'test_datas_w2v.npy', 'wb'), w2v_test_data)
```


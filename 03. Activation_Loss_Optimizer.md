# 다양한 종류의 Activation, Loss function과 Optimizer 소개

<br/><br>

## 1. Activation Function(활성화 함수)
<br/>

<img src="https://user-images.githubusercontent.com/31339365/103432250-92788080-4c1f-11eb-96e1-a2631248e39d.png"></img>

<br/>

### 1.1. Sigmoid
> 입력층의 결과가 일정값 이상이면 1을, 아니면 0을 리턴하는 함수로, **최종 출력(output layer)의 결과가 0과 1의 이진분류(binary classification)에 많이 사용**된다.

### 1.2. ReLU (Rectified Linear Unit)
> 역전파문제로 **숨은층(hidden layer)에서 sigmoid 함수를 대신해 사용**하는 함수이다. <br/>
> 입력값이 0보다 적으면 0을 출력하고, 0보다 크면 그대로 출력한다. <br/>
> 수식(y=max(0,x))이 간단하여, 다른 활성화 함수들에 비해 처리속도가 빠름 <br/>

### 1.3. ELU (Exponential Linear Unit)
> ReLU의 장점을 모두 포함하고, 단점을 개선한 비교적 최신 방법 <br/>
> 단점은, ReLU에 비해 처리속도가 좀 더 걸린다.

### 1.4. Tanh(Hyperbolic Tangent), Softsign
> 잘 사용하지 않지만, **RNN, LSTM 학습에 사용**됨 <br/>
> Softsign은 tanh를 대체하기 위해 고안된 함수로, tanh 대신에 사용 가능

### 결론
> * 가장 먼저 ReLU를 사용한다.
> * 성능 향상을 목적으로 Leak ReLU, Maxout, ELU를 시도해 본다.
> * Tanh는 가급적 피한다.
> * Sigmoid는 이진 출력층에서 사용하고 숨은층에서는 사용하지 않는다.

```python
print(dir(tf.keras.activations))

## 결과
[...
 'elu',
 'exponential',
 'gelu',
 'get',
 'hard_sigmoid',
 'linear',
 'relu',
 'selu',
 'serialize',
 'sigmoid',
 'softmax',
 'softplus',
 'softsign',
 'swish',
 'tanh']
```

<br/><br>

## 2. Lost Function(손실 함수)
> 손실함수는 아래 그림에서 알 수 있듯이 실제값과 예측값과의 차이를 구하는 함수로, 가까울수록 가중치를 주거나 멀수록 가중치를 주는 다양한 방법을 취한다.

<img src="https://user-images.githubusercontent.com/31339365/103432957-d07ba180-4c2b-11eb-8abe-83a0e6a114a6.png"></img>

<br/>

### 2.1. MSE(Mean Squared Error, 평균제곱오차)
> 제곱을 하기 때문에 값이 커질수록 가중치가 커지며, 값이 항상 양수로 계산된다. <br/>
> 딥러닝에서 **회귀(regression) 모델의 손실함수로 많이 사용**된다.

### 2.2. MAE(Mean Absolute Error, 평균절대오차)
> 딥러닝에서 **회귀(regression) 모델의 손실함수로 많이 사용**된다.

### 2.3. Crossentropy
> 불확실성, 정보량, 예측확률, 실제확률 등으로 설명하는데, 잘 이해하지 못하는 부분이다. <br/>
> 머신러닝, 특히 **딥러닝에서 분류문제의 손실함수로 사용**한다.
> * BinaryCrossentropy - 이진분류
> * CategoricalCrossentropy - 다중분류(라벨이 one-hot encoding 형태로 제공될 때 사용)
> * SparseCategoricalCrossentropy - 다중분류(라벨이 0, 1, 2, 3, 4 와 같은 형태로 제공될 때 사용)

### 2.4. 그외
> 회귀용
> * MSE, MAE, MSLE, MAPE, KLD, Poisson, Logcosh, Cosine Similarity, Huber <br/>

> 분류용
> * BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, Hinge, Squared Hinge, Categorical Hinge

```python
print(dir(tf.keras.losses))

## 결과
['BinaryCrossentropy',
 'CategoricalCrossentropy',
 'CategoricalHinge',
 'CosineSimilarity',
 'Hinge',
 'Huber',
 'KLD',
 'KLDivergence',
 'LogCosh',
 'Loss',
 'MAE',
 'MAPE',
 'MSE',
 'MSLE',
 'MeanAbsoluteError',
 'MeanAbsolutePercentageError',
 'MeanSquaredError',
 'MeanSquaredLogarithmicError',
 'Poisson',
 'Reduction',
 'SparseCategoricalCrossentropy',
 'SquaredHinge',
 'binary_crossentropy',
 'categorical_crossentropy',
 'categorical_hinge',
 'cosine_similarity',
 'deserialize',
 'get',
 'hinge',
 'huber',
 'kl_divergence',
 'kld',
 'kullback_leibler_divergence',
 'log_cosh',
 'logcosh',
 'mae',
 'mape',
 'mean_absolute_error',
 'mean_absolute_percentage_error',
 'mean_squared_error',
 'mean_squared_logarithmic_error',
 'mse',
 'msle',
 'poisson',
 'serialize',
 'sparse_categorical_crossentropy',
 'squared_hinge']
```

<br/><br>

## 3. Optimizer Function(최적화 함수)
> 손실함수는 아래 그림에서 알 수 있듯이 실제값과 예측값과의 차이를 구하는 함수로, 가까울수록 가중치를 주거나 멀수록 가중치를 주는 다양한 방법을 취한다.

<img src="https://user-images.githubusercontent.com/31339365/103434258-0b88cf80-4c42-11eb-8362-79a62de08365.png"></img>

```python
print(dir(tf.keras.optimizers))

## 결과
['Adadelta',
 'Adagrad',
 'Adam',
 'Adamax',
 'Ftrl',
 'Nadam',
 'Optimizer',
 'RMSprop',
 'SGD',
 'deserialize',
 'get',
 'schedules',
 'serialize']
```


# Tensorflow 2.x
<br/>

> Tensorflow는 2.x 버전에서 많은 변화를 가져왔다. <br/>
> 여기서는 2.x에서의 주요 변경점 위주로 설명한다. <br/>
> 주요 내용은 tf.keras.layer, 모델구축 방법 및 모델학습/검증 방법, 텐서보드 사용법 등이다.

<br/>

### 주요 변경점
##### ⓐ API 통폐합 
> 초기 버전인 1.x에서는 다양한 서브 라이브러리를 개발하는 것에 초점을 두어, 각 라이브러리들이 유사한 기능을 수행하는 함수들을 각각 만들어 사용했고, 당연히 개발자들은 유사한 함수들을 사용하는데 힘들었었는데 2.x 버전에서 이를 정리했다고 보면 된다.

##### ⓑ Eager Execution Mode (즉시실행모드) 지원
> 1.x에서는 그래프를 만들고, 세션을 할당해 그래프를 실행하는 방식이였는데, 2.x의 즉시실행모드로 실행되기 때문에 그래프를 만들면 바로 값을 확인할 수 있다.  이를 통해 불필요한 상용구 코드(boilerplate code)를 줄여주고 모델 디버깅을 더욱 쉽게해 준다.

##### ⓒ 모델구축 방법 변경
> 1.x에서는 입출력 텐서와 모델 처리용 텐서를 만들기 위해 반복적인 코드가 많이들어갔는데, 2.x부터는 keras를 활용해 모델을 구축하면서 사용하기 간편할뿐더러 매우 유연하고 높은 성능을 보이는 모델구축 방법을 제공한다. <br/>
> "2. 모델구축 방법 변경"을 참고하기 바란다.

##### ⓓ 모델학습 방법 변경
> 모델의 학습, 검증, 예측하는 방법에 대해 텐서플로 2.0 공식 가이드에서는 아래와 같이 권장한다. <br/>
> 아래 "3. 모델학습 방법 변경"에서 케라스 내장 API를 이용해 모델을 학습하고 검증 및 예측하는 전 과정에 대해 확인하기 바란다.

<br/><br/>

## 1. tf.keras.layers
> 텐서플로를 이용해 딥러닝 모델을 만드는 것은, 블록을 쌓아서 전체 구조를 만들어 가는 것과 비슷하다. <br/>
> 따라서 텐서플로는 블록을 쉽게 만들고, 변경하며, 조합할 수 있는 기능을 제공한다.  <br/>
> 다양한 방법이 있지만 텐서플로 2.0 이후에는 대부분 tf.keras.layers의 모듈로 통합하여 표준으로 사용하고 있다.

<br/>

### 1.1. tf.keras.layers.Dense
> 신경망 구조의 가장 기본적인 형태인 "층"을 만드는 함수이다.
```python
# python 1.x
W = tf.Variable(tf.random_uniform([5, 10], -1.0, 1.0))
b = tf.Variable(tf.zeros([10]))
y = tf.matmul(W, input) + b

# python 2.x
y = tf.keras.layers.Dense(unit=10, kernel_initializer='random_uniform', activation=tf.matmul)(input)
```

##### 중요 인자들(초기값)
> * units : 출력 값의 크기(라벨 수), integer 혹은 long 형
> * activation : activation function
> * use_bias(True) : bias 사용 여부
> * kernel/bias_initializer(glorot_uniform/zeros) : 가중치/bias 초기화 함수
> * kernel/bias/activity_regularizer(None/None/None) : 가중치/bias/출력값 정규화 방법

<br/>

> 10개의 노드를 가지는 은닉층과 최종 출력값이 2개인 신경만 구조 만들기 예:
```python
INPUT_SIZE = (20, 1)
inputs = tf.keras.layers.Input(shape=INPUT_SIZE)
hidden = tf.keras.layers.Dense(units=10, activation='relu')(inputs)       # activation func 지정 방법 1
output = tf.keras.layers.Dense(units=2, activation=tf.nn.sigmoid)(hidden) # activation func 지정 방법 2 'sigmoid'
```

<br/><br/>

### 1.2. tf.keras.layers.Dropout
> 신경망 모델의 대표 문제점인 과적합(overfitting)은 정규화(regularization)을 통해 해결한다.  Dropout은 과적합을 해결하는 정규화의 대표적인 방법으로, 전체 입력값 중에서 특정 값을 0으로 만들어 모델에서 사용할 수 없도록 만든다.
```python
INPUT_SIZE = (20, 1)
inputs = tf.keras.layers.Input(shape=INPUT_SIZE)
dropout = tf.keras.layers.Dropout(rate=0.2)(input)
hidden = tf.keras.layers.Dense(units=10, activation='relu')(dropout)      
output = tf.keras.layers.Dense(units=2, activation=tf.nn.sigmoid)(hidden) 
```

##### 중요 인자들(초기값)
> * rate : dropout 비율(0~1), tf.nn.dropout에서 rate는 dropout을 하지 않을 비율을 나타냄
> * noise_shape : dropout을 특정 값에만 적용할 수 있음
> * seed : 모든 ML에 나오는 seed는 random값에 적용해 함수가 동일한 random값을 가지도록 한다. 여기서는 동일한 값을 dropout 시키기 위해 사용함

<br/><br/>

### 1.3. tf.keras.layers.Conv1/2/3D
> 합성곱(Convolution) 연산 모델("층", Dense와 유사)을 생성하며, 1/2/3 차원의 입/출력 값을 처리한다.
```python
INPUT_SIZE = (1, 28, 28)
inputs = tf.keras.layers.Input(shape=INPUT_SIZE)
conv1d = tf.keras.layers.Conv1D(filters=10, kernel_size=3, padding='same', activation='relu')
```

##### 중요 인자들(초기값)
> * filter : 필터의 갯수로, 정수형이며, 출력의 차원수를 나타냄
> * kernel_size : 필터의 크기로, kernel_size=2로 하면 필터 갯수는 (kernel_size * filter)가 됨
> * strides(1) : 옆으로 이동할 크기, 1이 아닌 값을 지정할 경우 dilation_rate는 1만 지정 가능
> * dilation_rate(1) : 원본 입력 옆으로 확장할 크기, 1이 아닌 값을 지정할 경우 strides는 1만 지정 가능
> * padding(valid) : 합성곱 연산을 하면, filter와 strides에 의해 출력 이미지는 입력 이미지보다 줄어드는데, 이를 방지하기 위해 연산 전에 filter와 strides를 감안해 입력 이미지 주위를 임의의 값으로 체우는 것을 padding이라고 함. same는 입출력 크기가 같고, valid는 출력이 적은 경우를 나타냄
> * activation, use_bias, kernel/bias_initializer, kernel/bias/activity_regularizer 등은 Dense와 동일

<br/><br/>

### 1.4. tf.keras.layers.MaxPool1/2/3D
> Conv1/2/3D 연산 출력인 피처 맵(feature map)의 크기를 줄이거나 중요한 특징을 뽑아내는 방법으로 합성곱 연산 후에 실행한다.  보통 최대값을 뽑는 max-pooling과 평균값을 뽑는 average-pooling이 있는데, max-pooling을 많이 사용한다.
> MaxPool 결과값은 행렬값으로 완전 연결 계층(Fully Connected Layer)에서 바로 사용할 수 없기 때문에 Flatten 처리를 해 백터값으로 변환해야 한다.
```python
INPUT_SIZE = (1, 28, 28)
inputs = tf.keras.layers.Input(shape=INPUT_SIZE)
conv1d = tf.keras.layers.Conv1D(filters=10, kernel_size=3, padding='same', activation='relu')
max_pool = tf.keras.layers.MaxPool1D(pool_size=3, padding='same')(conv1d)
flatten = tf.keras.layers.Flatten()(max_pool)   # Flatten은 별다른 인자가 없이 바로 사용함
hidden = tf.keras.layers.Dense(units=50, activation=tf.nn.relu)(flatten)
output = tf.keras.layers.Dense(units=10, activation='softmax')(hidden)
```

##### 중요 인자들(초기값)
> * pool_size(2) : 풀링을 적용할 필터의 크기
> * strides(1), padding(valid)는 Conv1/2/3D와 동일

<br/><br/>

## 2. 모델구축 방법 변경
> 텐서플로 2.x는 케라스를 이용해 모델을 구축할 것을 권장하며, 케라스는 아래와 같은 방법을 제공한다.
> * Sequential API
> * Functional API
> * Sequential + Functional API
>   * Custom Layers
> * Subclassing (Custom Model)

<br/>

### 2.1. Sequential API
> 가장 기본적인 방법으로 순차적으로 모델을 구축한다. 구현 자체가 매우 간단해 많이 사용하지만 모델의 층들이 순차적으로 구성되어 있지 않는 경우 사용할 수 없다. 아래의 경우에는 다음에 나오는 다른 방법을 사용해야 한다.
> * 다중 입력값 모델(Multi-Input Models), 다중 출력값 모델(Multi-Output Models), 공유층을 활용하는 모델, 데이터의 흐름이 순차적이지 않는 모델
```python
import tensorflow as tf
from tensorflow.keras.layers import Dense

# 방법 1
s_model1 = tf.keras.Sequential()
s_model1.add(Dense(64, activation='relu'))
s_model1.add(Dense(10, activation='softmax'))

# 방법 2
s_model2 = tf.keras.Sequential([
  Dense(64, activation='relu'),
  Dense(10, activation='softmax')
])
```

<br/><br/>

### 2.2. Functional API
> Functional API를 활용하기 위해서는 1.x에서 사용하던 tf.placeholder와 같은 Input을 선언해 모델의 입력부를 정의해야하며, 원하는 모든 형태를 추가할 수 있기 때문에 Sequential API 방법보다  다양하게 활용할 수 있다.
> 실제 사용시에는 간단한 Sequential 방법과 마지막에 소개할 Subclassing 방법을 가장 많이 사용한다.
```python
import tensorflow as tf
from tensorflow.keras.layers import Dense

input = tf.keras.Input(shape=(32,))
hidden = Dense(64, activation='relu')(input)
output = Dense(10, activation='softmax')(hidden)
```

<br/><br/>

### 2.3. Custom Layer
> layers.Layer를 상속받아 클래스를 만들기 때문에 __init__(), build(), call() 함수를 정의해 줘야 한다.
> 다음에 소개할 Subclassing(Custom Model)보다 불필요한 코딩 부분이 더 많다.
```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Layer

class CustomLayer(Layer):
  
  def __init__(self, hidden_dim, output_dim):
    self.hidden_dim = hidden_dim
    self.output_dim = output_dim
    super(CustomLayer, self).__init__()
    
  def build(self, input_shape):                               # input_shape ??
    self.hidden = Dense(self.hidden_dim, activation='relu')
    self.output = Dense(self.output_dim, activation='softmax')
    
  def call(self, inputs):
    _hidden = self.hidden(inputs)
    _output = self.output(_hidden)
    return _output
    
model = tf.keras.Sequential([CustomLayer(64, 10)])
```

<br/><br/>

### 2.4. Subclassing (Custom Model)
> tf.keras.Model를 상속받아 클래스를 만들기 때문에 __init__(), call() 함수를 정의해 줘야 한다.
> 가장 많이 사용한다.
```python
import tensorflow as tf
from tensorflow.keras.layers import Dense

class CustomModel(tf.keras.Model):
  
  def __init__(self, hidden_dim, output_dim):
    super(CustomModel, self).__init__(name='my_model')        # my_model은 이름이기 때문에 공백이나 특수문자를 사용할 수 없음
    self.hidden = Dense(hidden_dim, activation='relu')
    self.output = Dense(output_dim, activation='softmax')
    
  def call(self, inputs):
    _hidden = self.hidden(inputs)
    _output = self.output(_hidden)
    return _output
    
model = tf.keras.Sequential([CustomModel(64, 10)])
```

<br/><br/>

## 3. 모델학습 방법 변경
> 텐서플로 2.x는 홈페이지 공식 가이드를 통해 아래와 같이 모델을 학습하라고 권장한다.
> * 케라스 모델의 내장 API 이용(예: model.fit(), model.evaluate(), model.predict()) - **실제로 가장 많이 사용한다.**
> * 학습, 검증 및 예측 등 모든 과정을 GradientTape 객체를 활용해 직접 구현. - 사용자 정의 형태로 어려워 보이긴 함.

<br/>

> 아래는 케라스 모델의 내장 API를 이용해 모델의 학습, 검증 및 예측을 하는 방법을 소개하면서, 텐서플로 2.x를 이용해 데이터를 전처리하고 마지막에 검증하는 전 프로세스를 설명한다.

### 3.1. 라이브러리 추가
```python
import tensorflow as tf
import numpy as np
import datetime

from tensorflow.keras import preprocessing, backend, layers
```
<br/>

### 3.2. NLP 처리를 위한 데이터 준비
```python
samples = [      
    '너 오늘 이뻐 보인다.',
    '나는 오늘 기분이 더러워',
    '끝내주는데, 좋은 일 있나봐?',
    '나 좋은 일 생겼어',
    '아 오늘 진짜 짜증나!',
    '환상적인데, 정말 좋은거 같아'
]
targets = [[1], [0], [1], [1], [0], [1]]
```
<br/>

### 3.3. 데이터 전처리
```python
token_dic = preprocessing.text.Tokenizer()            # Tokenizer 정의
token_dic.fit_on_texts(samples)                       # samples를 이용해 token_dic 생성
soc_data = token_dic.texts_to_sequences(samples)      # token_dic을 이용해 samples의 문자를 숫자로 변환(리스트 형태, 구분자 ',')
input_data = np.array(soc_data)                       # 모델에 들어갈 형태(np.array, 리스트 구분자 ' ')로 변경
labels = np.array(targets)                            # 모델에 들어갈 형태(np.array, 리스트 구분자 ' ')로 라벨 변경
word_index = token_dic.word_index                     # token_dic에 생성된 word_index 확인용 

print(f'input_data:\n{input_data}')
print(f'word_index:\n{word_index}')
```
<br/>

### 3.4. 하이퍼파라메타 지정
```python
BATCH_SIZE = 2
EPOCHS = 100

VOCAB_SIZE = len(word_index) + 1                      # 사전크기
EMBED_SIZE = 128                                      # 임베딩 층 크기
HIDDEN_DIM = 256                                      # 히든 층 차원(노드) 갯 수
OUTPUT_DIM = 1                                        # 출력 층 노드 갯 수
```
<br/>

### 3.5. Subclassing(Custom Model)을 이용한 모델 정의
```python
class CustomModel(tf.keras.Model):
  
  def __init__(self, vocab_size, embed_size, hidden_dim, output_dim):
    super(CustomModel, self).__init__(name='myModel')
    self.embedding = layers.Embedding(vocab_size, embed_size)
    self.hidden = layers.Dense(hidden_dim, activation='relu')
    self.output = layers.Dense(output_dim, activation='sigmoid')
    
  def call(self, inputs):
    _hidden = self.embedding(inputs)
    _hidden = tf.reduce_mean(_hidden, axis=1)
    _hidden = self.hidden(_hidden)
    _hidden = self.output(_hidden)
    return _hidden
```
<br/>

### 3.6. 모델 생성 및 학습
```python
model = CustomModel(vocab_size=VOCAB_SIZE,                # 모델 생성 1단계
                    embed_size=EMBED_SIZE, 
                    hidden_dim=HIDDEN_DIM, 
                    output_dim=OUTPUT_DIM)
model.compile(optimizer=tf.keras.optimizers.Adam(0.001),  # 모델 생성 2단계
              loss='binary_crossentropy'                  # output_dimension=1인 경우 사용, 
              metrics=['accuracy']                        #  '0' '1' 바이너리 값에 특화됨
model.summary()                                           # 생성된 모델 확인             
model.fit(input_data, labels, epochs=EPOCHS, batch_size=BATCH_SIZE) # 모델 학습
```
<br/>

### 3.7. 모델 검증 (좀 더 확인 필요)
```python
predict_samples = [
    '너 오늘 이뻐 보인다.',
    '나는 오늘 기분이 더러워'
]
predict_soc = token_dic.texts_to_sequences(predict_samples) # 전처리 했을 때 사용한 token을 그대로 사용
predict_data = np.array(predict_soc)
predict = model.predict(predict_data)
print(predict)
```

<br/><br/>

## 4. TensorBoard 사용
> 텐서플로 2.x에서는 텐서보드를 사용하는 방법이 쉽다.
```python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']
log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')                 # 로그 저장 디렉토리 생성
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)  # 로그 저장 콜백함수 생성
model.fit(input_sequences, labels, epochs=EPOCHS, batch_size=BATCH_SIZE, 
          callbacks=[tensorboard_callback])                                               # 모델 학습시 콜백함수 추가
%tensorboard --logdir logs/fit                               # 텐서보드 보기, 터미널에서는 '%' 빼고 치면됨          
```
<br/>

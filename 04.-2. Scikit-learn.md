# Scikit-learn
<br/>

> 사이킷런은 파이썬용 머신러닝 라이브러리로, 머신러닝 기술을 활용하는데 필요한 다양한 기능을 제공한다. <br/>
> 지도학습과 비지도학습 지원, 모델 선택과 평가, 데이터 변환 및 데이터 불러오기, 계산성능 향상 모듈로 구성되어 있다. <br/>

<br/>

> 여기서는 붓꽃 데이터를 이용해 데이터 불러오기, 전처리 및 지도/비지도 학습을 순차적으로 진행한다.

<br/><br/>

## 1. 데이터 전처리
> 붓꽃(Iris) 데이터는 1936년에 만들어진 머신러닝 기초를 공부할 때 가장 많이 사용하는 데이터로써 매우 직관적이고 사용하기 쉽다. <br/><br/>
> **자주 사용하는 데이터 셋**
> * load_iris : 붓꽃 데이터
> * load_boston : 보스톤 집값 데이터
> * load_diabetes : 당뇨병 환자 데이터
> * load_digits : 손글씨 데이터
> * load_linnerud : Multi-output regression용 데이터
> * load_wine : 와인 데이터
> * load_breast_cancer : 위스콘신 유방암 환자 데이터

<br/>

### 1.1. 데이터 불러오기 및 데이터 확인
```python
from sklearn.datasets import load_iris

iris_dataset = load_iris()

print(f'iris_dataset type: {type(iris_dataset)}')
print(f'iris_dataset key: {iris_dataset.keys()}')
print(f'iris_dataset[data] type: {type(iris_dataset["data"])}')

print(f'iris_dataset[data] size: {iris_dataset["data"].shape}')

## 결과
# iris_dataset type: <class 'sklearn.utils.Bunch'>
# iris_dataset key: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])
# iris_dataset[data] type: <class 'numpy.ndarray'>

# iris_dataset[data] size: (150, 4)       
```
> sklearn.utils.Bunch라는 자료구조이며, 이는 다음과 같은 데이터를 가지고 있다.
> * data : 샘플 데이터, Numpy 배열
> * target : Label 데이터, Numpy 배열
> * feature_names : 특성 이름 정보
> * target_names : Label 데이터 이름 정보
> * DESCR : 데이터 설명
> * filename : 다운된 데이터 셋 파일 저장 위치(확장자: csv)

<br/>

### 1.2. 데이터 전처리
> 여기서는 간단히 사이킷런의 사용법에 대해서만 설명할 예정이라 간단히 데이터를 학습과 검증용으로 분리하는 부분만 소개한다.
```python
from sklearn.model_selection import train_test_split
train_data, test_data, train_label, test_label = \      
  train_test_split(iris_dataset['data'], iris_dataset['target'], test_size=0.3, random_state=42)                     

print(f'type train_data: {type(train_data)}')
print(f'train_data.shape: {train_data.shape}, test_data.shape: {test_data.shape}')

## 결과
# type train_data: <class 'numpy.ndarray'>
# train_data.shape: (105, 4), test_data.shape: (45, 4)
```
> 데이터 분리는 train_test_split로 수행하며, 사용방법이 직관적이니 자세한 설명은 생략한다. <br/>
> 참고로, 입력되는 데이터와 라벨은 Numpy 배열이고, random_state는 다른 부분에서 사용하는 seed와 동일한 기능을 수행한다.

<br/><br/>

## 2. 지도 학습
> 지도학습에서 사용되는 머신러닝 알고리즘은 다양하기 때문에 알고리즘 선택에 신중해야 한다. <br/>
> 알고리즘은 주어진 데이터의 종류와 인공지능 모델로 하고자하는 일이 무엇인가에 따라 선택하는 대상이 달라진다.<br/>
> 사이킷런에서 추천하는 [올바른 알고리즘 선택을 위한 지도](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)를 참고하기 바란다.

<br/>

### 2.1. 모델 선택 및 학습
> 여기서는 아래 내용을 위 지도에 대입해 "k-최근접 이웃 분류기(k-Nearest Neighbor Classifier)"를 선택했다.
> * 선택조건: 데이터 크기 150개, 3개로 분류된 라벨이 있는 데이터를 가지며, 숫자형 데이터이다. <br/>

> 참고로, k-NN을 사용할 때 데이터가 많거나 k값이 커지면 성능에 문제가 있으니 피하는 것이 좋으며, 이 부분도 위 지도를 확인해보면 알 수 있다.
```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=1)           #1
knn.fit(train_data, train_label)                    #2
```
> * #1) 모델을 정의하는 부분으로 "n_neighbors=1"은 가장 가까운 데이터 하나의 값을 그대로 사용한다는 의미이며, 이 값이 3이면 가장 가까운 3개의 값 중에 가장 많이 나오는 값을 선택한다는 의미이다. 3가지 값이 모두 다른 경우 가장 가까운 대상의 값을 취한다.
> * #2) 학습용 데이터를 이용해 모델을 학습하는 부분이다.

<br/>

### 2.2. 모델 시험
> 모델 시험은 학습된 모델에 시험용 데이터를 라벨이 없이 제공한 후 예측한 결과를 실제 라벨과 비교하는 과정이다.
```python
import numpy as np

predict_labels = knn.predict(test_data)             #1
print(np.mean(predict_labels==test_label))          #2

## 결과
# 1.0                                               #3
```
> * #1) 학습된 모델에 시험용 데이터를 제공해 라벨을 예측하게 한다. <br/>
> * #2) 예측된 라벨과 실제 라벨을 비교해 정확도를 확인하는 부분으로, np.mean은 두 배열의 동일 인덱스의 값이 같으면 1을, 다르면 0을 리턴한 값의 평균을 구한다. <br/>
> * #3) 결과로 정확도가 100%를 의미한다.

<br/>

### 2.3. 모델 확인
> 실제 임의의 데이터를 모델에 적용해 어떤 결과가 나오는지 확인해 보자.
```python
sample_input = np.array([[2.1, 2.8, 1.7, 1.2], [6.4, 5.3, 3.2, 5.4]])   #1
predict_sample = knn.predict(sample_input)          #2
print(f'predict_sample: {predict_sample}')          #3

## 결과
# predict_sample: [0 2]                             #4
```
> * #1) 임의의 데이터 생성(0~10까지의 임의의 실수로 구성된 4차원 배열) <br/>
> * #2) 모델을 통해 예측 작업 수행 <br/>
> * #3) 결과 출력 <br/>
> * #4) 0, 2라는 라벨을 출력

<br/><br/>

## 3. 비지도 학습
> 비지도 학습은 라벨이 없는 데이터에 대해서 수행하는 인공지능 모델 학습 방법으로, 굳이 있는 라벨을 없다고 가정하고 비지도 학습을 수행한다.

<br/>

### 3.1. 모델 선택 및 학습
> 여기서는 아래 내용을 위 지도에 대입해 "k-평균 군집화(k-Means Clustering)"를 선택했다. 정확하게는 "Mini-Batch k-Means Clustering"이지만 간단히 k-Means로 실습한다.
> * 선택조건: 데이터 크기 150개, 3개로 분류할 수 있는 라벨이 없는 데이터다. <br/>
```python
from sklearn.cluster import KMeans

k_means = KMeans(n_clusters=3)                      #1
k_means.fit(train_data)                             #2

for i in range(k):
    print(f'{i} cluster: {train_label[k_means.labels_ == i]}')  # 분류된 군집 확인
    
## 결과                                               #3
# 0 cluster: [2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2]
# 1 cluster: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
# 2 cluster: [1 2 1 1 1 1 1 2 1 1 1 2 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1]
```
> * #1) 분류가 3개이니 k값을 3으로 지정한다. <br/>
> * #2) 학습용 데이터를 이용해 모델을 학습하는 부분이다. **여기에는 라벨이 들어가지 않는다.** <br/>
> * #3) 이 결과에 나오는 0~2는 3개의 군집을 의미하며, 위에서 제공되는 라벨과는 다르다. 가장 많은 있는 라벨을 해당 군집의 라벨로 정한다.
>   * 0 cluster: label=2, 1 cluster: label=0, 2 cluster: label=1

<br/>

### 3.2. 모델 시험
> 모델 시험은 학습된 모델에 시험용 데이터를 라벨이 없이 제공한 후 예측한 결과를 실제 라벨과 비교하는 과정이다.
```python
import numpy as np

predict_cluster = k_means.predict(test_data)        #1
np_arr = np.array(predict_cluster)                  #2
np_arr[np_arr==0], np_arr[np_arr==1], np_arr[np_arr==2] = 3, 4, 5   #3
np_arr[np_arr==3], np_arr[np_arr==4], np_arr[np_arr==5] = 2, 0, 1
print(np.mean(np.arr==test_label))                  #4

## 결과
# 0.9333333333333333                                #5
```
> * #1) 학습된 모델에 시험용 데이터를 제공해 라벨을 예측하게 한다.
> * #2~3) 예측된 군집에 라벨을 붙여주는 작업을 수행한다.
> * #4) 예측된 라벨과 실제 라벨을 비교한다. 일반적인 군집화에서는 이런 작업은 잘 하지 않는다.
> * #5) 예측결과 0.9333..으로 지도 학습보다는 정확도가 떨어진다.

<br/>

### 3.3. 모델 확인
> 지도학습과 유사하기 때문에 지도학습의 방법을 참고하고, 추가로 예측된 결과에 대해서 3.2의 #2~3을 수행후 확인한다.

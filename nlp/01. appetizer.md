# 시작하기 전에
<br/>

> NLP를 수행할 때 자주 사용하는 라이브러리 소개

<br/><br/>


## 1. Tensorflow (version 2.x)
> Tensorflow는 2.x 버전에서 많은 변화를 가져왔다. 주요 변경내용은 아래와 같다. <br/>

##### API 통폐합 
> 초기 버전인 1.x에서는 다양한 서브 라이브러리를 개발하는 것에 초점을 두어, 각 라이브러리들이 유사한 기능을 수행하는 함수들을 각각 만들어 사용했고, 당연히 개발자들은 유사한 함수들을 사용하는데 힘들었었는데 2.x 버전에서 이를 정리했다고 보면 된다.

<br/>

### 1.1. tf.keras.layers
> 텐서플로를 이용해 딥러닝 모델을 만드는 것은, 블록을 쌓아서 전체 구조를 만들어 가는 것과 비슷하다. 따라서 텐서플로는 블록을 쉽게 만들고, 변경하며, 조합할 수 있는 기능을 제공한다.  다양한 방법이 있지만 텐서플로 2.0 이후에는 대부분 tf.keras.layers의 모듈로 통합하여 표준으로 사용하고 있다.

<br/>

#### 1.1.1. tf.keras.layers.Dense
> 신경망 구조의 가장 기본적인 형태인 "층"을 만드는 함수이다.
```python
# python 1.x
W = tf.Variable(tf.random_uniform([5, 10], -1.0, 1.0))
b = tf.Variable(tf.zeros([10]))
y = tf.matmul(W, input) + b

# python 2.x
y = tf.keras.layers.Dense(unit=10, kernel_initializer='random_uniform', activation=tf.matmul)(input)
```
<br/>

> **중요 인자들**(초기값)
> * units : 출력 값의 크기(라벨 수), integer 혹은 long 형
> * activation : activation function
> * use_bias(True) : bias 사용 여부
> * kernel/bias_initializer(glorot_uniform/zeros) : 가중치/bias 초기화 함수
> * kernel/bias/activity_regularizer(None/None/None) : 가중치/bias/출력값 정규화 방법

<br/>

> 10개의 노드를 가지는 은닉층과 최종 출력값이 2개인 신경만 구조 만들기 예:
```python
INPUT_SIZE = (20, 1)
inputs = tf.keras.layers.Input(shape=INPUT_SIZE)
hidden = tf.keras.layers.Dense(units=10, activation='relu')(inputs)       # activation func 지정 방법 1
output = tf.keras.layers.Dense(units=2, activation=tf.nn.sigmoid)(hidden) # activation func 지정 방법 2 'sigmoid'
```

<br/>

#### 1.1.2. tf.keras.layers.Dropout
> 신경망 모델의 대표 문제점인 과적합(overfitting)은 정규화(regularization)을 통해 해결한다.  Dropout은 과적합을 해결하는 정규화의 대표적인 방법으로, 전체 입력값 중에서 특정 값을 0으로 만들어 모델에서 사용할 수 없도록 만든다.
```python
INPUT_SIZE = (20, 1)
inputs = tf.keras.layers.Input(shape=INPUT_SIZE)
dropout = tf.keras.layers.Dropout(rate=0.2)(input)
hidden = tf.keras.layers.Dense(units=10, activation='relu')(dropout)      
output = tf.keras.layers.Dense(units=2, activation=tf.nn.sigmoid)(hidden) 
```
<br/>

> **중요 인자들**(초기값)
> * rate : dropout 비율(0~1), tf.nn.dropout에서 rate는 dropout을 하지 않을 비율을 나타냄
> * noise_shape : dropout을 특정 값에만 적용할 수 있음
> * seed : 모든 ML에 나오는 seed는 random값에 적용해 함수가 동일한 random값을 가지도록 한다. 여기서는 동일한 값을 dropout 시키기 위해 사용함

<br/>

#### 1.1.3. tf.keras.layers.Conv1/2/3D
> 합성곱(Convolution) 연산 모델("층", Dense와 유사)을 생성하며, 1/2/3 차원의 입/출력 값을 처리한다.
```python
INPUT_SIZE = (1, 28, 28)
inputs = tf.keras.layers.Input(shape=INPUT_SIZE)
conv1d = tf.keras.layers.Conv1D(filters=10, kernel_size=3, padding='same', activation='relu')
```
<br/>

> **중요 인자들**(초기값)
> * filter : 필터의 갯수로, 정수형이며, 출력의 차원수를 나타냄
> * kernel_size : 필터의 크기로, kernel_size=2로 하면 필터 갯수는 (kernel_size * filter)가 됨
> * strides(1) : 옆으로 이동할 크기, 1이 아닌 값을 지정할 경우 dilation_rate는 1만 지정 가능
> * dilation_rate(1) : 원본 입력 옆으로 확장할 크기, 1이 아닌 값을 지정할 경우 strides는 1만 지정 가능
> * padding(valid) : 합성곱 연산을 하면, filter와 strides에 의해 출력 이미지는 입력 이미지보다 줄어드는데, 이를 방지하기 위해 연산 전에 filter와 strides를 감안해 입력 이미지 주위를 임의의 값으로 체우는 것을 padding이라고 함. same는 입출력 크기가 같고, valid는 출력이 적은 경우를 나타냄
> * activation, use_bias, kernel/bias_initializer, kernel/bias/activity_regularizer 등은 Dense와 동일
